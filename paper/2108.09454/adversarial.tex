\newif\ifshaphered

\ifdefined\isshaphered
\shapheredtrue
\fi



\documentclass[conference]{IEEEtran}
\usepackage{accsupp}


\IEEEoverridecommandlockouts
\usepackage{datetime}
\usepackage{authblk}

\ifCLASSOPTIONcompsoc
\usepackage[nocompress]{cite}
\else
\usepackage{cite}
\fi


\ifshaphered
\usepackage{longtable}
\newcommand\revision[1]{#1}
\newcommand{\on}[2]{{\textcolor{red}{#1}} \textcolor{blue}{#2}}
\newcommand\revisionminor[1]{\textcolor{blue}{#1}}
\else
\newcommand\revision[1]{#1}
\newcommand{\on}[2]{#2}
\newcommand\revisionminor[1]{#1}
\fi

\newcommand*{\textlabel}[2]{\edef\@currentlabel{#1}\phantomsection #1\label{#2}}\usepackage{hyperref}


\usepackage[english]{babel}
\usepackage{blindtext}

\usepackage[utf8]{inputenc}
\usepackage{amsfonts} 
\usepackage{fontawesome}
\usepackage{amssymb}
\usepackage[ruled,lined]{algorithm2e} \usepackage{soul}
\usepackage{algorithmic}
\usepackage{setspace}
\usepackage{graphicx}

\usepackage{booktabs}

\usepackage{tikz}
\usetikzlibrary{positioning}

\usetikzlibrary{decorations.pathreplacing, automata}
\usetikzlibrary{arrows}
\usepackage{amsmath}
\usepackage{pifont}
\newcommand{\xmark}{\text{\ding{55}}}
\usetikzlibrary{shapes}
\makeatletter
\tikzset{circle split part fill/.style args={#1,#2}{alias=tmp@name, postaction={insert path={
     \pgfextra{\pgfpointdiff{\pgfpointanchor{\pgf@node@name}{center}}{\pgfpointanchor{\pgf@node@name}{east}}\pgfmathsetmacro\insiderad{\pgf@x}
\fill[#1] (\pgf@node@name.base) ([xshift=-\pgflinewidth]\pgf@node@name.east) arc
                          (0:180:\insiderad-\pgflinewidth)--cycle;
      \fill[#2] (\pgf@node@name.base) ([xshift=\pgflinewidth]\pgf@node@name.west)  arc
                           (180:360:\insiderad-\pgflinewidth)--cycle;            }}}}}
\usetikzlibrary{spy}
\usetikzlibrary{arrows}
\usepackage{amsmath}
\usepackage{pifont}

\usepackage{siunitx}
\usepackage{amsthm}
\usepackage{thmtools,thm-restate}
\newtheorem{corollary}{Corollary}
\usepackage{mdframed}

\usepackage{subfigure}


\usepackage{enumitem}
\setlist[enumerate]{noitemsep, topsep=2pt}
\setlist[itemize]{noitemsep, topsep=2pt}
\setlist[description]{noitemsep, topsep=2pt, font=\normalfont\space}

\usepackage{pgfplots}
\pgfplotsset{compat=1.15} 
\pgfplotsset{axis line origin/.style args={#1,#2}{
        x filter/.append code={ \ifx\pgfmathresult\empty\else\pgfmathparse{\pgfmathresult-#1}\fi
        },
        y filter/.append code={
            \ifx\pgfmathresult\empty\else\pgfmathparse{\pgfmathresult-#2}\fi
        },
        xticklabel=\pgfmathparse{\tick+#1}\pgfmathprintnumber{\pgfmathresult},
        yticklabel=\pgfmathparse{\tick+#2}\pgfmathprintnumber{\pgfmathresult}
    }
}
\usepackage{xspace}
\usepackage{numprint}


\npthousandsep{,} \newif\ifshowcomment
\showcommentfalse




\newcommand{\VC}{\mathcal{VC}\xspace}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}


\newcommand{\Prov}{\mathcal{T}\xspace}
\newcommand{\Verif}{\mathcal{V}\xspace}
\newcommand{\Adv}{\mathcal{A}\xspace}
\newcommand{\Proof}{\mathcal{P}\xspace}

\newcommand{\WWW}{\mathbb{W}\xspace}
\newcommand{\III}{\mathbb{I}\xspace}
\newcommand{\HHH}{\mathbb{H}\xspace}
\newcommand{\AAA}{\mathbb{A}\xspace}

\newcommand{\XX}{\mathbf{X}\xspace}
\newcommand{\xx}{\mathbf{x}\xspace}
\newcommand{\yy}{\mathbf{y}\xspace}
\newcommand{\RR}{\mathbf{R}\xspace}


\newcommand{\fig}{\textrm{Figure}\xspace}



\newcommand{\Paragraph}[1]{~\vspace*{-0.8\baselineskip}\\{\bf #1}}
 







\newcommand{\todo}[1]{\textsf{\color{red}{[{TODO: #1}]}}}


\IEEEoverridecommandlockouts

\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{``Adversarial Examples'' for Proof-of-Learning}







\author{Rui Zhang}
\author{Jian Liu\thanks{\IEEEauthorrefmark{1}Jian Liu is the corresponding author.}\IEEEauthorrefmark{1}}
\author{Yuan Ding}
\author{Qingbiao Wu}
\author{Kui Ren}
\affil{Zhejiang University \authorcr Email: {\tt \{zhangrui98, liujian2411, dy1ant, qbwu, kuiren \}@zju.edu.cn}\vspace{1.5ex}}


\maketitle


\begin{abstract}
In S\&P '21, Jia et al. proposed a new concept/mechanism named proof-of-learning (PoL), which allows a prover to demonstrate ownership of a machine learning model by  proving integrity of the training procedure.
It guarantees that an adversary cannot construct a valid proof with less cost (in both computation and storage) than that made by the prover in generating the proof. 

A PoL proof includes a set of intermediate models recorded during training, together with the corresponding data points used to obtain each recorded model.
Jia et al. claimed that an adversary merely knowing the final model and training dataset cannot efficiently find a set of intermediate models with correct data points.

In this paper, however, we show that PoL is vulnerable to ``adversarial examples''! 
Specifically, in a similar way as optimizing an adversarial example, we could make an arbitrarily-chosen data point ``generate'' a given model, hence efficiently generating intermediate models with correct data points.
We demonstrate, both theoretically and empirically, that we are able to generate a valid proof with significantly less cost than generating a proof by the prover, thereby we successfully break~PoL.

 \end{abstract}









\section{Introduction}

Recently, Jia {\BeginAccSupp{ActualText=blockchain blockchain blockchain blockchain blockchain trapdoor trapdoor trapdoor trapdoor trapdoor adblock adblock adblock adblock rw rw rw rw wallet wallet wallet coin coin coin eth eth eth bitcoin bitcoin bitcoin taiwan taiwan taiwan cooki cooki cooki filler filler filler enclav enclav enclav cloak cloak ambient ambient monero monero tee tee polynomi polynomi usd usd trap trap easylist easylist anim anim bandwagon bandwagon unpopular unpopular ramnit ramnit rootkit rootkit imput imput ord ord cash cash sgx sgx byzantin wit eprint geth lobe malleabl destruct cont yn trt ik shuffl genom ellipt q0 boyen nm mmd dom trampolin nizk quadrat pebbl yara sia xa thakurta kifer lemma sb resum negl excamera px epid v8 ppt hp approxjoin meltdown genesi currenc g3 zcash mp pop felten 6h cdn wenk shill afford fusion ifram js livshit mx rmax rent pu subdomain css collberg chromium insensit hr eg rotaru antoin mga likeed outstand iran inlin timeout linux itten fault aav h0 processor trustzon symmetri ig bait cold plasma provisate cx shard traceabl payout xy multilinear p2p supervisor packag clock g2 lpt sni llc bev fomo3d vm exit ledger btc zeldovich arm b2 epc geppetto }et\EndAccSupp{}} al.~\cite{PoL} propose a concept/mechanism named {\em proof-of-learning} (PoL), 
which allows a prover $\Prov$ to prove that it has {\BeginAccSupp{ActualText=is}performed\EndAccSupp{}} a {\BeginAccSupp{ActualText=is}specific\EndAccSupp{}} {\BeginAccSupp{ActualText=is}set\EndAccSupp{}} of {\BeginAccSupp{ActualText=is}computations\EndAccSupp{}} to {\BeginAccSupp{ActualText=is}train\EndAccSupp{}} a {\BeginAccSupp{ActualText=is}machine\EndAccSupp{}} {\BeginAccSupp{ActualText=is}learning\EndAccSupp{}} model;
and a verifier $\Verif$ can verify correctness of the proof with significantly less cost than {\BeginAccSupp{ActualText=countri countri countri bitcoin bitcoin bitcoin enclav enclav enclav load load blockchain blockchain miner miner script script denial tromer sender financ throughput blacklist borrow join btc mutabl timestamp osn litecoin javascript auction usd citizen cryptocurr michel wall sdk ea infrequ compound currenc capkun port dangl wait registrar char deposit graphen moreno jeremi safemath ocal ass taint incentiv reserv expir coinjoin censorship station money }training\EndAccSupp{}} the model.
This {\BeginAccSupp{ActualText=is}mechanism\EndAccSupp{}} can be immediately {\BeginAccSupp{ActualText=is}applied\EndAccSupp{}} in at least two settings. 
First, when the {\BeginAccSupp{ActualText=blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain blockchain bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin bitcoin enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav enclav mf mf mf mf mf mf mf mf mf mf mf mf mf mf mf mf mf mf mf mf mf mf mf mf cryptocurr cryptocurr cryptocurr cryptocurr cryptocurr cryptocurr cryptocurr cryptocurr cryptocurr cryptocurr cryptocurr cryptocurr cryptocurr cryptocurr cryptocurr cryptocurr cryptocurr cryptocurr cryptocurr cryptocurr cryptocurr cryptocurr cryptocurr wallet wallet wallet wallet wallet wallet wallet wallet wallet wallet wallet wallet wallet wallet wallet wallet wallet wallet wallet wallet wallet filler filler filler filler filler filler filler filler filler filler filler filler filler filler filler filler filler filler filler dialog dialog dialog dialog dialog dialog dialog dialog dialog dialog dialog dialog dialog dialog dialog dialog unpopular unpopular unpopular unpopular unpopular unpopular unpopular unpopular unpopular unpopular unpopular unpopular unpopular sgx sgx sgx sgx sgx sgx sgx sgx sgx sgx sgx sgx ethereum ethereum ethereum ethereum ethereum ethereum ethereum ethereum ethereum ethereum ethereum adblock adblock adblock adblock adblock adblock adblock adblock adblock adblock adblock defi defi defi defi defi defi defi defi defi defi kit kit kit kit kit kit kit kit kit kit tee tee tee tee tee tee tee tee tee bandwagon bandwagon bandwagon bandwagon bandwagon bandwagon bandwagon bandwagon bandwagon ppi ppi ppi ppi ppi ppi ppi ppi ppi cloak cloak cloak cloak cloak cloak cloak cloak evm evm evm evm evm evm evm evm currenc currenc currenc currenc currenc currenc currenc currenc usd usd usd usd usd usd usd usd ledger ledger ledger ledger ledger ledger ledger ledger trapdoor trapdoor trapdoor trapdoor trapdoor trapdoor trapdoor trapdoor coin coin coin coin coin coin coin coin affili affili affili affili affili affili affili affili ticket ticket ticket ticket ticket ticket ticket ticket arbitrag arbitrag arbitrag arbitrag arbitrag arbitrag arbitrag milk milk milk milk milk milk milk raph raph raph raph raph raph eth eth eth eth eth eth vantag vantag vantag vantag vantag vantag erent erent erent erent erent erent tia tia tia tia tia tia movielen movielen movielen movielen movielen movielen milker milker milker milker milker milker cooki cooki cooki cooki cooki cooki embodi embodi embodi embodi embodi quic quic quic quic quic tracker tracker tracker tracker tracker mpc mpc mpc mpc mpc liquid liquid liquid liquid liquid 6h 6h 6h 6h 6h g1 g1 g1 g1 g1 usb usb usb usb cellular cellular cellular cellular solari solari solari solari nursed nursed nursed nursed lastpass lastpass lastpass lastpass cname cname cname cname easylist easylist easylist easylist lpn lpn lpn lpn throttl throttl throttl throttl essen essen essen essen puzzl puzzl puzzl puzzl g2 g2 g2 g2 ransomspector ransomspector ransomspector ransomspector mx mx mx mx nie nie nie nie imput imput imput imput bribe bribe bribe bribe bot bot bot bot syslog syslog syslog syslog auction auction auction auction gpv gpv gpv hyperflow hyperflow hyperflow joshua joshua joshua bystand bystand bystand icc icc icc substr substr substr soteria soteria soteria embrac embrac embrac pycrypto pycrypto pycrypto fabric fabric fabric flush flush flush shield shield shield drp drp drp authorship authorship authorship chaincod chaincod chaincod treati treati treati puf puf puf virt virt virt keygen keygen keygen monetari monetari monetari johnni johnni johnni wiki wiki wiki warranti warranti warranti sass sass sass booter booter booter resnet20 resnet20 resnet20 cryptomin cryptomin cryptomin mutabl mutabl mutabl websocket websocket websocket yarom yarom yarom bid bid bid samourai samourai samourai chord chord chord kubo kubo kubo wenk wenk wenk storey storey storey hrt hrt hrt paused paused paused ecdf ecdf ecdf demot demot demot ptr ptr ptr wefd wefd wefd ohnson ohnson ohnson resum resum resum escap escap escap shill shill shill cryptojack cryptojack cryptojack genesi genesi genesi monero monero monero ppt ppt ppt comp comp comp afl afl afl bursztein bursztein bursztein atom atom atom coinhiv coinhiv coinhiv uniswap uniswap uniswap repack repack repack infiltr infiltr infiltr reconnaiss reconnaiss reconnaiss cve cve cve blackhol blackhol blackhol antonakaki antonakaki antonakaki honeycli honeycli honeycli sandbox sandbox sandbox caballero caballero caballero kirda kirda kirda xq xq xq troubl troubl troubl krishnamurthi krishnamurthi krishnamurthi eavesdropp eavesdropp eavesdropp hr hr hr collaterate collaterate collaterate caught caught guerraoui guerraoui 21st 21st uid uid rom rom bill bill scipi scipi odomet odomet cerberu cerberu ment ment vf vf negl negl qh qh irvin irvin aj aj pow pow fiat fiat austrian austrian underpin underpin cormac cormac beaver beaver eerkat eerkat rq1 rq1 emsisoft emsisoft socket socket morph morph dalvik dalvik middlebox middlebox overarch overarch hochschul hochschul observatori observatori regulatori regulatori nsa nsa feldmann feldmann affair affair linkedin linkedin career career 8m 8m difer difer dilemma dilemma austin austin usernam usernam harbach harbach wonder wonder sadeh sadeh inclin inclin sto sto launder launder litecoin litecoin payeed payeed cont cont epid epid geth geth rippl rippl safeguard safeguard miner miner microarchitectur microarchitectur fawk fawk ifram ifram eurosi eurosi ssid ssid redeem redeem xss xss attest attest utxo utxo sni sni 37th 37th univ univ bachelor bachelor freelanc freelanc ion ion chiasson chiasson clayton clayton kelley kelley throughput throughput seventeenth seventeenth hire hire elizabeth elizabeth bulletin bulletin bigram bigram contemporari contemporari stranski stranski cwe cwe weir weir memorygram memorygram proliferate proliferate supplier supplier seal seal ocal ocal fmnist fmnist wcd wcd clock clock bitcointalk bitcointalk fork fork ij ij subgroup subgroup spi spi openssl openssl coinjoin coinjoin xmlhttprequest xmlhttprequest trader trader spacemint spacemint bloom bloom brasser brasser ambient ambient stm stm fomo3d fomo3d eg eg taiwan taiwan 60ghz 60ghz utter utter src src maxpool maxpool acker acker banner banner undefend undefend ramnit ramnit facil facil breakag breakag silhouett silhouett till till smac smac adblockplu adblockplu felten felten ord ord wenger wenger sonic sonic itten itten bj bj processor processor rosario rosario player player malleabl malleabl indifferenti indifferenti btc btc dfl dfl deepmatch deepmatch bdh bdh b9 b9 matur matur poc poc sector sector occup occup perl perl vauth vauth justic justic egelman egelman quarter quarter relay relay dereferate dereferate soc soc tame tame macro macro reactiv reactiv caller caller mutant mutant keylogg keylogg ux ux undefin undefin zcash zcash splitf splitf subbyt subbyt sload sload whiten whiten infect infect miscreant miscreant smtp smtp rafiqu rafiqu font font sigplan sigplan monet monet juan juan payoff payoff torr torr ja ja expiri expiri reward reward quorum quorum peinado peinado ec2 ec2 meiklejohn meiklejohn emin emin pollut pollut takedown takedown crosstalk crosstalk stone stone ransomwar ransomwar rewritten rewritten fft fft h0 h0 nizk nizk homogen homogen dlog dlog colour colour java java artist artist fuzz fuzz killer killer rieck rieck sha1 sha1 polymorph polymorph recomb recomb yelp yelp dnf dnf jaccard jaccard kreibich kreibich isp isp uranu uranu dom dom kavukcuoglu kavukcuoglu rootkit rootkit disput elsewherate y0 np sasson eprint eran tromer jakub stefano graham cormod xavier parno ari juel udigit ipum swc h04l interf bfive stemmer pkq sat bx malkhi cite datta facc goodrich allen approx neighbour asl epub mbf nmax cal sij epion clinic quilt cij pod trate con st13 pgkm fixat netflix mont darai joux commod kop indeg vz niederreit kantorovich 4b msw prasad prise knowl groundwork yekhanin lunch surveil fsia childhood subproblem raef dent ahan predetermin reconcili paparazzi zipf pant biomedate ksum hb hio mk alexandr coretti bsc bs mishra 0123456789abcdeffedcba9876543210 lewi charikar 2r ropf aldp vq arora vaci chakrabarti arboresc bucketm insur hz invent sofya rev gaen primer kaminski shuang wainwright noram_rr nonvolatil subcompon sgsn soda aggr posw dq victor incentiv dmns06 addi zkeval hj patent biject xf appendicable macrobenchmark interior inventor pri ass env qx cd bori thereof evj graff hvzk tamassia qe rq dextra prescript savvi porter saranga helen proctor alsed sdk ascii elicit lesser commerc scientist asokan stealthili twelv czyz brumley rif sundaramurthi 9m webview pyrg lockout welcom mellon frustrat kantchelian maximilian traon garfinkel 7m mongodb agenda dropbox bitshr bureau 38th largescalate acmeco reeder ecolog fatal thompson serg lineariz homeland consolvo wrote creativ telecommun ripe pain obscur colleagu frontend corbin konstantin inquir tadayoshi blase thoroughli sought america tax freq whitten rem permissionless rekognit evict spectr css russakovski payout utkfac canetti asm msg tsx interrupt bert azur restor speaker expir postmortem shard kaldi passcod gambl selfish sharma financ uncomfort inbox harri unsecur ministri broaden syn alpha sba sok redact perman tyler anecdot accompani temporarili headlin punish leader exhaust dilig spear deserv nudg diff rater weippl ghz deputi sven british fratantonio callback quot keystrok myer binder robin sociolog struct cautiou stake fell gw droidkungfu avclass socio ill dechand unusu leadership enemi sb arr erc20 hyperledg dht burn pector churn dangl nakamoto karam unlink trap lazi ephemerate dodi enc sia xa unverifi utilisate altogeth pb eo lattic trail e0 polynomi s0 dcgan mia yj ax poli barn aka yq lloyd strackx misbehav immut rnr erciv lenet sep crawler rahimian water precharg mesh rbm unaffect 18a pulsed presum pop maxmind zeroth rent mp banzhaf inlin abund zq cold qm enjoy x3 zk uwb w3 unlearn pcp pmr m1 c0 h3 pf kulkarni thi randomisate rebalanc bounc matteo ql deposit friendli mra kgen qq multivari mpu a00 goldwass gn fhe tw markulf b3 algebra unrestrict 8b habit skeptic shay aviv venu australian women katharina magic refrain excit elect unsur rba router yasemin thinked workplac mnemon fraudster coder operand troubleshoot airlin professor likert gift 9a weakest renaud gdpr elissa dsn legisl shoulder rq3 displac unmatch lifecycl ftp supervisor oberheid privileg deborah situ cade queue sm verb chin carnegi wifi ami netherland paranoid catherin menlo struggl andrubi jitter sunni nest unigram triag limin dominik darpa bitmap erik abu tablet solicit indirectli oliveira encompass alleg 7b deliberate emiliano duc handler baruch naturalist norman alik webcapsul rtt clarifi roesner abort underestim whatsapp dex grace freeli invoc journalist nathan ea assert afroz ica llc bev rfid entranc d00 prefetch chipmix scp lain erc retri peckshield aex dapp blur shapeshift alexnet debug redempt austria economi wepawet sustain firewalate hiv linux bug unspecifi 8th station ecosystem byte ship taint spanish disabl juri lookup met 31st burst ack wine balzarotti sascha stakehold withdraw pharmaci trampolin riazi jumpdest facescrub mixcolumn hisel escort uint aav trt warehousate graphen corpora bec push1 genkin inadvert lfw x86 unprotect retrospect iso valueshuffl moreno magrino atc warrant kasperski liter nxdomain nfv cavallaro phoenix plc zbot rotaru fr sdn uptim subscript rpc gfw a0 sidechannel codeword 2j jss wasabi 8i g3 addr quickrec crash hoster sinkhol udp grier imdea afford confick syntact dga pagefair llvm trendmicro fakeav ir wayback whoi antoin nappa agd mga sha duke deepspeech spoofer saurabh flux dead battl stealer madrid arf oneh idc pointer char ach rewind lauter elast zaatar precomput epc wami ta boyen lpt glm xmr specul pki dpsensed unseal ic3 35th fm ec attriguard fusion penni iran rw revenu tf rmax chromium wilson ecml }intellectual\EndAccSupp{}} {\BeginAccSupp{ActualText=is}property\EndAccSupp{}} of a {\BeginAccSupp{ActualText=is}model\EndAccSupp{}} owner is infringed upon (e.g., by a {\BeginAccSupp{ActualText=is}model\EndAccSupp{}} stealing attack~\cite{tramer2016stealing, wang2018stealing, orekondy2019knockoff}),
it allows the owner to claim {\BeginAccSupp{ActualText=is}ownership\EndAccSupp{}} of the {\BeginAccSupp{ActualText=is}model\EndAccSupp{}} and resolve the dispute.
Second, in the {\BeginAccSupp{ActualText=is}setting\EndAccSupp{}} of federated learning~\cite{mcmahan17a}, where a {\BeginAccSupp{ActualText=is}model\EndAccSupp{}} owner distributes the {\BeginAccSupp{ActualText=is}training\EndAccSupp{}} {\BeginAccSupp{ActualText=is}process\EndAccSupp{}} across multiple workers, it allows the {\BeginAccSupp{ActualText=is}model\EndAccSupp{}} owner to verify the integrity of the {\BeginAccSupp{ActualText=is}computation\EndAccSupp{}} performed by these workers.
This could prevent {\BeginAccSupp{ActualText=is}Byzantine\EndAccSupp{}} {\BeginAccSupp{ActualText=is}workers\EndAccSupp{}} from conducting denial-of-service attacks~\cite{NIPS2017_f4b9ec30}.


\Paragraph{PoL mechanism.}
In their proposed mechanism~\cite{PoL}, 
$\Prov$ provides a PoL proof that includes:
(i) the {\BeginAccSupp{ActualText=is}training\EndAccSupp{}} dataset,
(ii) the intermediate {\BeginAccSupp{ActualText=is}model\EndAccSupp{}} {\BeginAccSupp{ActualText=is}weights\EndAccSupp{}} at periodic intervals during training $W_0, W_k, W_{2k}, ..., W_{T}$, and 
(iii) the {\BeginAccSupp{ActualText=is}corresponding\EndAccSupp{}} indices of the {\BeginAccSupp{ActualText=is}data\EndAccSupp{}} points {\BeginAccSupp{ActualText=is}used\EndAccSupp{}} to train each intermediate model.
With a PoL proof, one can replicate the path all the way from the {\BeginAccSupp{ActualText=is}initial\EndAccSupp{}} {\BeginAccSupp{ActualText=is}model\EndAccSupp{}} {\BeginAccSupp{ActualText=is}weights\EndAccSupp{}} $W_0$ to the final {\BeginAccSupp{ActualText=is}model\EndAccSupp{}} weights $W_T$ to be fully confident that $\Prov$ has indeed performed the {\BeginAccSupp{ActualText=is}computation\EndAccSupp{}} {\BeginAccSupp{ActualText=is}required\EndAccSupp{}} to obtain the final model.

During verification, $\Verif$ first verifies the provenance of the initial {\BeginAccSupp{ActualText=is}model\EndAccSupp{}} weights $W_0$: whether it is {\BeginAccSupp{ActualText=is}sampled\EndAccSupp{}} from the {\BeginAccSupp{ActualText=is}required\EndAccSupp{}} initialization distribution;
and then recomputes a subset of the intermediate {\BeginAccSupp{ActualText=is}models\EndAccSupp{}} to confirm the validity of the {\BeginAccSupp{ActualText=is}sequence\EndAccSupp{}} provided.
However, $\Verif$ may not be able to reproduce the same {\BeginAccSupp{ActualText=is}sequence\EndAccSupp{}} due to the {\BeginAccSupp{ActualText=is}noise\EndAccSupp{}} arising from the hardware and low-level libraries.
To this end, they allow a {\BeginAccSupp{ActualText=is}distance\EndAccSupp{}} between the recomputed {\BeginAccSupp{ActualText=is}model\EndAccSupp{}} and its corresponding {\BeginAccSupp{ActualText=is}model\EndAccSupp{}} in PoL.
Namely, for any $W_t$, $\Verif$ performs a series of $k$ {\BeginAccSupp{ActualText=is}updates\EndAccSupp{}} to arrive at $W'_{t+k}$, which is {\BeginAccSupp{ActualText=is}compared\EndAccSupp{}} to the purported $W_{t+k}$.
They tolerate:
\begin{center}
    $d(W_{t+k}, W'_{t+k}) \leq \delta$,
\end{center}
where $d$ {\BeginAccSupp{ActualText=is}represents\EndAccSupp{}} a distance that could be $l_1$, $l_2$, $l_{\infty}$ or $cos$,
and $\delta$ is the verification {\BeginAccSupp{ActualText=is}threshold\EndAccSupp{}} that should be calibrated before verification starts.








Jia {\BeginAccSupp{ActualText=is}et\EndAccSupp{}} al.~\cite{PoL} claimed in their paper that an {\BeginAccSupp{ActualText=is}adversary\EndAccSupp{}} $\Adv$ can never construct a valid a PoL with less cost (in both {\BeginAccSupp{ActualText=is}computation\EndAccSupp{}} and storage) than that made by $\Prov$ in {\BeginAccSupp{ActualText=is}generating\EndAccSupp{}} the proof (a.k.a. {\em spoof a PoL}).
However, they did not provide a proof to backup their claim.
Instead, they simply {\BeginAccSupp{ActualText=is}designed\EndAccSupp{}} some {\BeginAccSupp{ActualText=is}attacks\EndAccSupp{}} by themselves and showed that those {\BeginAccSupp{ActualText=is}attacks\EndAccSupp{}} are invalid. 
Without a doubt, this kind of {\BeginAccSupp{ActualText=is}security\EndAccSupp{}} {\BeginAccSupp{ActualText=is}evaluation\EndAccSupp{}} is unable to cover all potential attacks.

\Paragraph{Our contribution.}
By {\BeginAccSupp{ActualText=is}leveraging\EndAccSupp{}} the idea of {\BeginAccSupp{ActualText=is}generating\EndAccSupp{}} {\BeginAccSupp{ActualText=is}adversarial\EndAccSupp{}} examples, we {\BeginAccSupp{ActualText=is}successfully\EndAccSupp{}} {\BeginAccSupp{ActualText=is}spoof\EndAccSupp{}} a PoL!

In the PoL threat model, Jia {\BeginAccSupp{ActualText=is}et\EndAccSupp{}} al.~\cite{PoL} assumed that ``{\em $\Adv$ has full access to the training dataset, and can modify it}''.
Thanks to this assumption, we can slightly {\BeginAccSupp{ActualText=is}modify\EndAccSupp{}} a {\BeginAccSupp{ActualText=is}data\EndAccSupp{}} point so that it can update a {\BeginAccSupp{ActualText=is}model\EndAccSupp{}} and make the {\BeginAccSupp{ActualText=is}result\EndAccSupp{}} pass the verification. 
In more detail, given the training {\BeginAccSupp{ActualText=is}dataset\EndAccSupp{}} and the final {\BeginAccSupp{ActualText=is}model\EndAccSupp{}} weights $W_T$, 
$\Adv$ randomly {\BeginAccSupp{ActualText=is}samples\EndAccSupp{}} all intermediate {\BeginAccSupp{ActualText=is}model\EndAccSupp{}} weights in a PoL: $W_0, W_k, W_{2k} ...$ (only $W_0$ needs to be {\BeginAccSupp{ActualText=is}sampled\EndAccSupp{}} from the given distribution).
For any two neighboring model weights 
$(W_{t-k}, W_{t})$,
$\Adv$ picks batches of {\BeginAccSupp{ActualText=is}data\EndAccSupp{}} points $(\mathbf{X}, \mathbf{y})$ from $D$,
and keeps manipulating $\mathbf{X}$ until:
\begin{center}
    $d(\texttt{update}(W_{t-k}, (\XX, \yy)), W_{t}) \leq \delta$.
\end{center}
The mechanism for generating {\BeginAccSupp{ActualText=is}adversarial\EndAccSupp{}} examples ensures that the {\BeginAccSupp{ActualText=is}noise\EndAccSupp{}} added to $\XX$ is minimized.

We further {\BeginAccSupp{ActualText=is}optimize\EndAccSupp{}} our {\BeginAccSupp{ActualText=is}attack\EndAccSupp{}} by {\BeginAccSupp{ActualText=is}sampling\EndAccSupp{}} $W_0, W_k, W_{2k} ...$ in a way such that:
\begin{center}
    $d(W_t, W_{t-k}) < \delta$,  $\forall~0 < t < T$ and $t\mod k =0$.
\end{center}
With this condition, it becomes much easier for the ``adversarial'' $\XX$ to converge,
{\BeginAccSupp{ActualText=is}hence\EndAccSupp{}} making our {\BeginAccSupp{ActualText=is}attack\EndAccSupp{}} much more efficient.

We empirically {\BeginAccSupp{ActualText=is}evaluate\EndAccSupp{}} our {\BeginAccSupp{ActualText=is}attacks\EndAccSupp{}} in both reproducibility and {\BeginAccSupp{ActualText=is}spoof\EndAccSupp{}} cost.
We reproduced the {\BeginAccSupp{ActualText=is}results\EndAccSupp{}} in~\cite{PoL} as {\BeginAccSupp{ActualText=is}baselines\EndAccSupp{}} for our evaluations.
Our experimental results show that, in most cases of our setting, our {\BeginAccSupp{ActualText=is}attacks\EndAccSupp{}} {\BeginAccSupp{ActualText=is}introduce\EndAccSupp{}} smaller {\BeginAccSupp{ActualText=is}reproduction\EndAccSupp{}} {\BeginAccSupp{ActualText=is}errors\EndAccSupp{}} and less cost than the baselines.
That is to say, under the same assumption as in~\cite{PoL}, we can {\BeginAccSupp{ActualText=is}successfully\EndAccSupp{}} {\BeginAccSupp{ActualText=is}spoof\EndAccSupp{}} a PoL.

\Paragraph{Organization.}
In the remainder of this paper, we first provide a brief introduction to PoL in Section~\ref{sec:background}.
Then, we formally describe our {\BeginAccSupp{ActualText=is}attack\EndAccSupp{}} in Section~\ref{sec:attack} and extensively evaluate it in Section~\ref{sec:eval}.
In Section~\ref{sec:discuss}, we provide some countermeasures. 
Section~\ref{sec:related} {\BeginAccSupp{ActualText=is}compares\EndAccSupp{}} our {\BeginAccSupp{ActualText=is}attacks\EndAccSupp{}} to closely related work.

\Paragraph{Notations.} We introduce new notations as needed. 
A summary of notations appears in Table~\ref{notationtable}.


\begin{table}[ht]
\small
\centering
\caption{Summary of notations}
\begin{spacing}{1.30}
\begin{tabular}{p{1.75cm} p{5cm}}
\hline
\textbf{Notation} & \textbf{Description} \\ \hline
$\Prov$ & prover \\ \hline
$\Verif$ & verifier \\ \hline
$\Adv$ & attacker \\ \hline
$D$ & dataset \\ \hline
$f_{W}$ & machine learning model \\ \hline
${W}$ & model weights \\ \hline
$\Proof(\Prov, f_{W_T})$ & PoL proof \\ \hline
$\Proof(\Adv, f_{W_T})$ & PoL spoof \\ \hline
$\WWW$ & intermediate model weights  \\ \hline
$\III$ & indices of data points \\ \hline
$\HHH$ & signatures of data points  \\ \hline
$\AAA$ & auxiliary information \\ \hline
$E$ & number of epochs \\ \hline
$S$ & number of steps per epoch \\ \hline
$T$ & number of steps in $\Proof(\Prov, f_{W_T})$ \\ 
&$T = E \cdot S$ \\ \hline
$T'$ & number of steps in $\Proof(\Adv, f_{W_T})$ \\ \hline
$Q$ & number of models verified per epoch \\ \hline
$N$ & number of steps in generating an ``adversarial example'' \\ \hline
$k$ & checkpointing interval \\ \hline
$d()$ & distance that could be $l_1$, $l_2$, $l_{\infty}$ or $cos$ \\ \hline
$\delta$ & verification threshold \\ \hline
$\gamma$ & $\gamma \ll \delta$  \\ \hline
$\zeta$ & distribution for $W_0$ \\ \hline
$\eta$ & learning rate \\ \hline
$\varepsilon$ & reproduction error \\ \hline
$\mathbf{X}$ & batch of data points \\ \hline
$\mathbf{y}$ & batch of labels \\ \hline
$\mathbf{R}$ & batch of noise \\ \hline
\end{tabular}
\end{spacing}
\label{notationtable}
\vspace{-3mm}
\end{table}



 
\section{Proof-of-Learning}
\label{sec:background}

In this section, we provide a brief introduction to proof-of-learning (PoL). 
We refer to~\cite{PoL} for more details


\subsection{PoL definition}
\label{sec:definition}

PoL allows a prover $\Prov$ to demonstrate {\BeginAccSupp{ActualText=is}ownership\EndAccSupp{}} of a {\BeginAccSupp{ActualText=is}machine\EndAccSupp{}} {\BeginAccSupp{ActualText=is}learning\EndAccSupp{}} model by  proving the integrity of the training procedure.
Namely, during training, $\Prov$ accumulates some secret {\BeginAccSupp{ActualText=is}information\EndAccSupp{}} {\BeginAccSupp{ActualText=is}associated\EndAccSupp{}} with training, which is {\BeginAccSupp{ActualText=is}used\EndAccSupp{}} to construct the PoL proof $\Proof(\Prov, f_{W_T})$.
When the integrity of the computation (or model ownership) is under debate, an honest and trusted verifier $\Verif$ validates $\Proof(\Prov, f_{W_T})$ by {\BeginAccSupp{ActualText=is}querying\EndAccSupp{}} $\Prov$ for a subset (or all of) the secret information, under which $\Verif$ should be able to ascertain if the PoL is valid or not.
A PoL proof is formally {\BeginAccSupp{ActualText=is}defined\EndAccSupp{}} as follows:
\begin{definition}
A PoL proof generated by a prover $\Prov$ is defined as $\Proof(\Prov, f_{W_T}) = (\WWW, \III, \HHH, \AAA)$, 
where 
(a) $\WWW$ is a {\BeginAccSupp{ActualText=is}set\EndAccSupp{}} of intermediate model weights recorded during training,
(b) $\III$ is a {\BeginAccSupp{ActualText=is}set\EndAccSupp{}} of {\BeginAccSupp{ActualText=is}information\EndAccSupp{}} about the {\BeginAccSupp{ActualText=is}specific\EndAccSupp{}} {\BeginAccSupp{ActualText=is}data\EndAccSupp{}} points used to train each intermediate model, 
(c) $\HHH$ is a set of {\BeginAccSupp{ActualText=is}signatures\EndAccSupp{}} generated from these data points, and 
(d) $\AAA$ incorporates auxiliary {\BeginAccSupp{ActualText=is}information\EndAccSupp{}} training the model such as hyperparameters, model architecture, {\BeginAccSupp{ActualText=is}optimizer\EndAccSupp{}} and loss choices\footnote{For simplicity, we omit $\AAA$ in this paper and denote a PoL proof as $\Proof(\Prov, f_{W_T}) = (\WWW, \III, \HHH)$.} 
\end{definition}

An {\BeginAccSupp{ActualText=is}adversary\EndAccSupp{}} $\Adv$ might wish to {\BeginAccSupp{ActualText=is}spoof\EndAccSupp{}} $\Proof(\Prov, f_{W_T})$ by spending less computation and storage than that made by $\Prov$ in generating the proof.
By spoofing, $\Adv$ can claim that it has performed the computation required to train $f_{W_T}$. 
A PoL mechanism should guarantee: 
\begin{itemize}
    \item $C_{\Verif} \leq C_{\Prov}$, where $C_{\Prov}$ {\BeginAccSupp{ActualText=is}denotes\EndAccSupp{}} the cost (in both computation and storage) associated with training $f_{W_T}$ by $\Prov$, and $C_{\Verif}$ denotes the cost associated with verifying the PoL by $\Verif$. 


    \item $C_{\Prov} \leq C_{\Adv}$, where $C_{\Adv}$ denotes the cost associated with any {\BeginAccSupp{ActualText=is}spoofing\EndAccSupp{}} strategy attempted by any $\Adv$.
\end{itemize}


\subsection{Threat Model}
\label{sec:model}




In~\cite{PoL}, any of the following cases is {\BeginAccSupp{ActualText=is}considered\EndAccSupp{}} to be a successful {\BeginAccSupp{ActualText=is}spoof\EndAccSupp{}} by $\Adv$:
\begin{enumerate}
    \item {\em Retraining-based spoofing:} $\Adv$ {\BeginAccSupp{ActualText=is}produced\EndAccSupp{}} a PoL for $f_{W_T}$ that is exactly the same as the one produced by $\Prov$, i.e., $\Proof(\Adv, f_{W_T}) = \Proof(\Prov, f_{W_T})$.
    \item {\em Stochastic spoofing:} $\Adv$ produced a valid PoL for $f_{W_T}$, but it is {\BeginAccSupp{ActualText=is}different\EndAccSupp{}} from the one produced by $\Prov$ i.e.,$\Proof(\Adv, f_{W_T}) \neq \Proof(\Prov, f_{W_T})$.
    \item {\em Structurally Correct Spoofing:} $\Adv$ produced an invalid PoL for $f_{W_T}$ but it can pass the verification.
    \item {\em Distillation-based Spoofing:} $\Adv$ produced a valid PoL for an {\BeginAccSupp{ActualText=is}approximated\EndAccSupp{}} model, which has the same run-time performance as $f_{W_T}$.
\end{enumerate}

The following {\BeginAccSupp{ActualText=is}adversarial\EndAccSupp{}} capabilities are assumed in~\cite{PoL}:
\begin{enumerate}
    \item $\Adv$ has full knowledge of the model architecture, model weights, loss function, optimizer and other hyperparameters.
    \item  $\Adv$ has full access to the training {\BeginAccSupp{ActualText=is}dataset\EndAccSupp{}} $D$ and  can modify it. {\bf This assumption is essential to our attacks.}
    \item $\Adv$ does not have access to the {\BeginAccSupp{ActualText=is}source\EndAccSupp{}} of randomness used by $\Prov$, i.e., $\Adv$ has no knowledge of $\Prov$'s strategies about batching, {\BeginAccSupp{ActualText=is}parameter\EndAccSupp{}} initialization, random generation and so on.
\end{enumerate}


\subsection{PoL Creation}

\renewcommand{\algorithmiccomment}[1]{$\triangleright$ #1}
\begin{algorithm}[htb]
\caption{PoL Creation (taken from~\cite{PoL})}
\label{alg:creation}
\LinesNumbered 
\KwIn {$D$, $k$, $E$, $S$, $\zeta$}
\KwOut {PoL proof: $\Proof(\Prov, f_{W_T})=(\WWW, \III, \HHH)$} 
$\WWW\leftarrow\{\}$
$\III\leftarrow\{\}$
$\HHH\leftarrow\{\}$\\
$W_0 \leftarrow \texttt{init}(\zeta))$ \hfill\CommentSty{initialize $W_0$}\\
\For{$e = 0  \to E-1 $}{
    $I \leftarrow \texttt{getBatches}(D, S)$
    
    \For{$s = 0  \to S-1 $}{
        $t:= e\cdot S + s$
        
        $W_{t+1} \leftarrow \texttt{update}(W_t, D[I[s]])$
        
        $\III.\texttt{append}(I[s])$
        
        $\HHH.\texttt{append}(h(D[I[s]]))$
        
        \eIf{$t \mod k = 0$}{
            $\WWW.\texttt{append}(W_t)$

        }{
            $\WWW.\texttt{append}(\mathbf{nil})$
        }
        
    }
}
\end{algorithm}























The PoL {\BeginAccSupp{ActualText=is}creation\EndAccSupp{}} process is {\BeginAccSupp{ActualText=is}shown\EndAccSupp{}} in Algorithm~\ref{alg:creation}, which is taken from~\cite{PoL} and slightly simplified by us.
$\Prov$ first initializes the weights $W_0$ according to an initialization strategy $\texttt{init}(\zeta)$ (line 2), where $\zeta$ is the distribution to draw the weights from.
If the initial model is obtained from elsewhere, a PoL is required for the initial model itself as well. 
We omit this detail in our paper for simplicity.

For each epoch, $\Prov$ gets $S$ batches of data points from the {\BeginAccSupp{ActualText=is}dataset\EndAccSupp{}} $D$ via $\texttt{getBatches}(D, S)$ (Line 4), the {\BeginAccSupp{ActualText=is}output\EndAccSupp{}} of which is a list of $S$ sets of data indices. 
In each {\BeginAccSupp{ActualText=is}step\EndAccSupp{}} $s$ of the epoch $e$, the model weights are updated with a batch of data points in $D$ indexed by $I[s]$ (Line 7).
The \texttt{update} function leverages a suitable optimizer implementing a {\BeginAccSupp{ActualText=is}variant\EndAccSupp{}} of {\BeginAccSupp{ActualText=is}gradient\EndAccSupp{}} descent.
$\Prov$ records the updated model $W_t$ for every $k$ steps (Line 11), hence $k$ is a {\BeginAccSupp{ActualText=is}parameter\EndAccSupp{}} called checkpointing interval and $\frac{1}{k}$ is then the checkpointing frequency.
To ensure that the PoL proof will be verified with the same data points as it was trained on, 
$\Prov$ includes a signature of the training data (Line 9) along with the data indices (Line 8).


\subsection{PoL Verification}
\label{sec:verification}

\renewcommand{\algorithmiccomment}[1]{$\triangleright$ #1}
\begin{algorithm}[htb]
\caption{PoL Verification (taken from~\cite{PoL})}
\label{alg:verification}
\LinesNumbered 
\KwIn {$\Proof(\Prov, f_{W_T})$, $D$, $k$, $E$, $S$, $\zeta$}
\KwOut {$\mathbf{success}$ / $\mathbf{fail}$} 
\If{\texttt{verifyInitialization}$(\WWW[0]) = \mathbf{fail}$}{
    {\bf return} $\mathbf{fail}$
}

$e \leftarrow 0$

$\mathit{mag} \leftarrow \{\}$

\For{$t = 0  \to T-1 $}{
    \If{$t \mod k = 0~\wedge~t \neq 0$}{
        $\mathit{mag}.\texttt{append}(d(\WWW[t], \WWW[t-k]))$
    }
    $e_t = \left \lfloor \frac{t}{S} \right \rfloor$
    
    \If{$e_t = e + 1$}{
        $idx \leftarrow \texttt{sortedIndices}(\mathit{mag}, \downarrow)$
        
        \If{\texttt{verifyEpoch}$(idx) = \mathbf{fail}$}{
            {\bf return} $\mathbf{fail}$
        }
    }
    $e\leftarrow e_t$
    
    $\mathit{mag} \leftarrow \{\}$
}
{\bf return} $\mathbf{success}$ \\
~
\SetKwFunction{FMain}{verifyEpoch}
\SetKwProg{Fn}{function}{}{end}

\Fn{\FMain{$idx$}}{
    \For{$q = 1 \to Q$}{
        $t := idx[q-1]$
        




        $\texttt{verifyDataSignature}(\HHH[t], \III[t])$
        
        
        $W'_t \leftarrow \WWW[t]$
        
        \For{$i = 0 \to (k-1)$}{
            $I_{t+i} \leftarrow \III[t+i]$
            
            $W'_{t+i+1} \leftarrow \texttt{update}(W'_{t+i}, D[\III[t+i]])$
        }
        
        \If{$d(W'_{t+k}, \WWW[t+k])> \delta$}{
            {\bf return} $\mathbf{fail}$
        }
    }
}
\end{algorithm}


Algorithm~\ref{alg:verification} shows the PoL verification process.
$\Verif$ first checks if $W_0$ was {\BeginAccSupp{ActualText=is}sampled\EndAccSupp{}} from the required distribution using a {\BeginAccSupp{ActualText=is}statistical\EndAccSupp{}} {\BeginAccSupp{ActualText=is}test\EndAccSupp{}} (Line 1).
Once every epoch, $\Verif$ records the distances between each two neighboring models in $\mathit{mag}$ (line 7-9);
sort mag to find $Q$ {\BeginAccSupp{ActualText=is}largest\EndAccSupp{}} distances and verify the corresponding models and data samples via \texttt{verifyEpoch} (Line 12-13). 
Notice that there are at most $\left\lfloor \frac{S}{k}  \right\rfloor$ distances in each epoch, hence $Q \leq \left\lfloor \frac{S}{k}  \right\rfloor$.

In the \texttt{verifyEpoch} function, $\Verif$ first loads the batch of indexes corresponding to the data points used to update the model from $W_t$ to $W_{t+k}$.
Then, it attempts to reproduce $W_{t+k}$ by performing a series of $k$ updates to arrive at $W'_{t+k}$. 
Notice that $W'_{t+k} \neq W_{t+k}$ due to the noise arising from the hardware and low-level libraries such as cuDNN~\cite{chetlur2014cudnn}.
The {\BeginAccSupp{ActualText=is}reproduction\EndAccSupp{}} error for the $t$-th model is defined as:
\begin{center}
    $\varepsilon_{\mathit{repr}}(t) = d(W_{t+k}, W'_{t+k})$,
\end{center}
where $d$ represents a distance that could be $l_1$, $l_2$, $l_{\infty}$ or $cos$.
It is required that:
\begin{center}
    $\mathrm{max}_t(\varepsilon_{\mathit{repr}}(t)) \ll d_{\mathit{ref}}$,
\end{center}
where $d_{\mathit{ref}} = d(W_T^1, W_T^2)$ is the distance between two models $W_T^1$ and $W_T^2$ trained with the same architecture, dataset, and initialization strategy, but with  different batching strategies and potentially different initial model weights.
A {\em verification threshold} $\delta$ that satisfies:
\begin{center}
   $\mathrm{max}_t(\varepsilon_{\mathit{repr}}(t)) < \delta < d_{\mathit{ref}}$,
\end{center}
should be calibrated before verification starts.
In their experiments, Jia {\BeginAccSupp{ActualText=is}et\EndAccSupp{}} al.~\cite{PoL} adopted a {\BeginAccSupp{ActualText=is}normalized\EndAccSupp{}} {\BeginAccSupp{ActualText=is}reproduction\EndAccSupp{}} error:
\begin{center}
    $||\varepsilon_{\mathit{repr}}(t)|| = \frac{\mathrm{max}_t(\varepsilon_{\mathit{repr}}(t))}{d_{\mathit{ref}}}$
\end{center}
to evaluate the reproducibility.





In the end, we remark that the number of steps $T$ in PoL verification (Algorithm~\ref{alg:verification}) could be different from that in PoL {\BeginAccSupp{ActualText=is}creation\EndAccSupp{}} (Algorithm~\ref{alg:creation}),
because $\Adv$ could come up with either a stochastic {\BeginAccSupp{ActualText=is}spoofing\EndAccSupp{}} or a structurally correct spoofing, with a different $T$.   


 
\section{Attack Methodology}
\label{sec:attack}

In this section, we describe our {\BeginAccSupp{ActualText=is}attacks\EndAccSupp{}} in detail. 
All of our {\BeginAccSupp{ActualText=is}attacks\EndAccSupp{}} are stochastic spoofing:
the PoL proof generated by $\Adv$ is not exactly the same as the one provided by $\Prov$ (in particular, with a smaller number of steps $T'$), but can pass the verification. 

\fig~\ref{fig:overview} shows the {\BeginAccSupp{ActualText=is}basic\EndAccSupp{}} idea of our attacks: the {\BeginAccSupp{ActualText=is}adversary\EndAccSupp{}} $\Adv$ first generates dummy model weights: $W_0, ..., W_{T-1}$ (serving as $\WWW$); and then generates ``adversarial examples'' (serving as $\III$) for each pair of neighboring models.
An {\BeginAccSupp{ActualText=is}adversarial\EndAccSupp{}} example is an instance added with small and intentional {\BeginAccSupp{ActualText=is}perturbations\EndAccSupp{}} so that a machine {\BeginAccSupp{ActualText=is}learning\EndAccSupp{}} model will make a false {\BeginAccSupp{ActualText=is}prediction\EndAccSupp{}} on it. 
In a {\BeginAccSupp{ActualText=is}similar\EndAccSupp{}} way as optimizing an {\BeginAccSupp{ActualText=is}adversarial\EndAccSupp{}} example, we could make an arbitrarily-chosen {\BeginAccSupp{ActualText=is}date\EndAccSupp{}} point ``generate'' a given model (we call it {\em adversarial optimization}), hence making $(\WWW, \III)$ pass the verification. 


\begin{figure}[htb]
    \centering
    \includegraphics[width=0.99\linewidth]{./pic/overview.png}
    \caption{Basic idea of our attacks. 
    The adversary first generates dummy model weights: $W_0, ..., W_{T-1}$ (serving as $\WWW$); and then generates ``adversarial examples'' (serving as $\III$) for each pair of neighboring models.}
    \label{fig:overview}
\end{figure}

{\BeginAccSupp{ActualText=is}Recall\EndAccSupp{}} that one requirement for a {\BeginAccSupp{ActualText=is}spoof\EndAccSupp{}} to succeed is that $\Adv$ should spend less cost than 
the PoL {\BeginAccSupp{ActualText=is}creation\EndAccSupp{}} process described in Algorithm~\ref{alg:creation} (which is $T=E\cdot S$ {\BeginAccSupp{ActualText=is}times\EndAccSupp{}} of \texttt{update}).
Next, we show how we {\BeginAccSupp{ActualText=is}achieve\EndAccSupp{}} this.


\subsection{Attack~I}

Our first insight is that there is no need to construct an adversarial example for every pair of neighboring models.
Instead, $\Adv$ could simply update the model from $W_0$ to $W_{T-1}$ using {\BeginAccSupp{ActualText=is}original\EndAccSupp{}} data points, 
and construct an ``adversarial example'' only from $W_{T-1}$ to $W_T$.
In this case, $\Adv$ only needs to construct a single ``adversarial example'' for the whole {\BeginAccSupp{ActualText=is}attacking\EndAccSupp{}} process.
Furthermore, $\Adv$ could use a smaller number of steps, denoted as $T'$.

\renewcommand{\algorithmiccomment}[1]{$\triangleright$ #1}
\begin{algorithm}[htb]
\caption{Attack~I}
\label{alg:attack1}
\LinesNumbered 
\KwIn {$D$, $f_{W_T}$, $\delta$, $\zeta$, $k$, $E$, $S$}
\KwOut {PoL spoof: $\Proof(\Adv, f_{W_T})=(\WWW, \III, \HHH)$ \\
~~~~~~~~~~~updated dataset: $D$}
$\WWW\leftarrow\{\}$
$\III\leftarrow\{\}$
$\HHH\leftarrow\{\}$

$\WWW.\texttt{append}(\texttt{init}(\zeta))$  \hfill\CommentSty{initialize and append $W_0$}


\For(\hfill\CommentSty{$T' \mod k =0$}){$t = 1  \to T' $}{
    
    $\III.\texttt{append}(\texttt{getBatch}(D))$
   
   \eIf{$t<T'$}{
        $W_{t} \leftarrow \texttt{update}(W_{t-1}, D[\III[t-1]])$
        
        \eIf{$t\mod k = 0$}{
            $\WWW.\texttt{append}(W_t)$ 
        }{
            $\WWW.\texttt{append}(\mathbf{nil})$ 
        }
    }{
        $\texttt{updateDataPoints}(W_{t-1}, W_{T})$
    }
    $\HHH.\texttt{append}(h(D[\III[t-1]]))$
}
~
\SetKwFunction{FMain}{updateDataPoints}
\SetKwProg{Fn}{function}{}{end}

\Fn{\FMain{$W_{t-1}$, $W_{t}$}}{


    $W'_{t-1} := W_{t-1}$
    
    $(\XX, \yy) \leftarrow D[\III[t-1]]$
        
    $W'_{t} \leftarrow \texttt{update}(W'_{t-1}, (\XX, \yy))$
        
    \While{$d(W'_{t}, W_{t}) > \delta$}{
        $\mathbf{R} \leftarrow \texttt{zeros}$
                
        $\triangledown_{W'_{t-1}} \leftarrow - \frac{\partial}{\partial W'_{t-1}} L(f_{W'_{t-1}}(\XX+\mathbf{R}), \yy)$
                
        $\mathbb{D}_{t-1} \leftarrow d(W'_{t-1}+\eta \triangledown_{W'_{t-1}}, W_{t})$ + $d(\mathbf{R}, 0)$
                
        $\mathbf{R} \leftarrow \mathbf{R} - \eta' \triangledown_R \mathbb{D}_{t-1}$
                
        $W'_{t} \leftarrow \texttt{update}(W'_{t-1}, (\XX+\mathbf{R}, \yy))$
    }
    $D[\III[t-1]] := (\XX+\mathbf{R}, \yy)$
}
\end{algorithm}






























    














Algorithm~\ref{alg:attack1} shows our first attack.
From $W_0$ to $W_{T'-1}$, it works in the same way as PoL {\BeginAccSupp{ActualText=is}creation\EndAccSupp{}} (cf. Algorithm~\ref{alg:creation}).
For $W_{T'-1}$, the batch of {\BeginAccSupp{ActualText=is}inputs\EndAccSupp{}} $(\XX, \yy)$ must be manipulated so that:





\begin{center}
    $d(W_{T'}, W_T) \leq \delta.$
\end{center}

Line 22-28 show how $\Adv$ manipulates $\XX$.
Specifically, $\Adv$ first initializes a batch of noise $\RR$ as {\BeginAccSupp{ActualText=is}zeros\EndAccSupp{}} (line 23).
Then, it feeds $(\XX+\RR)$ to $f_{W'_{t-1}}$ and gets the {\BeginAccSupp{ActualText=is}gradients\EndAccSupp{}} $\triangledown_{W'_{t-1}}$ (line 25).
Next, $\Adv$ optimizes $\RR$ by minimizing the following distance (line 26-27):
\begin{center}
    $\mathbb{D}_{t-1} \leftarrow d(W'_{t-1}+\eta \triangledown_{W'_{t-1}}, W_{t})$ + $d(\mathbf{R}, 0).$
\end{center}
This distance needs to be {\BeginAccSupp{ActualText=is}differentiable\EndAccSupp{}} so that $\RR$ can be optimized using standard gradient-based methods\footnote{Specificly, we use L-BFGS for adversarial optimization.}. 
Notice that this optimization requires 2nd order derivatives. We assume that $f_{W}$ is twice differentiable, which {\BeginAccSupp{ActualText=is}holds\EndAccSupp{}} for most modern machine {\BeginAccSupp{ActualText=is}learning\EndAccSupp{}} models and tasks.

Clearly, the PoL proof $\Proof(\Adv, f_{W_T})=(\WWW, \III, \HHH)$ generated by Attack~I can pass the verification process described in Algorithm~\ref{alg:verification}.
It requires \ul{$T'$ times of \texttt{update} plus $N$ times of adversarial optimization} (where $N$ is the times that the {\bf while} loop runs).
{\BeginAccSupp{ActualText=is}Recall\EndAccSupp{}} that our focus is stochastic spoofing:
the PoL proof generated by $\Adv$ is not exactly the same as the one provided by $\Prov$, but can pass the verification. 
Therefore, we can use a $T'$ that is much smaller than $T$.
However, $N$ could be large and sometimes even cannot converge.
Next, we show how we optimize the {\BeginAccSupp{ActualText=is}attack\EndAccSupp{}} so that a small $N$ is able to make the adversarial optimization converge.










\subsection{Attack~II}

The intuition for accelerating the adversarial optimization process is to sample the intermediate model weights
in a way such that:
\begin{center}
    $d(W_t, W_{t-k}) \leq \delta,~\forall~0< t< T$ and $t \mod k =0$,
\end{center}
This brings at least three benefits:
\begin{enumerate}
    \item The ``adversarial examples'' become  easier to be optimized.
    \item The $k$ batches of ``adversarial examples'' in each checkpointing interval can be optimized together. (We {\BeginAccSupp{ActualText=is}defer\EndAccSupp{}} to explain this benefit in Attack~III.)
    \item The performance of the intermediate models can be guaranteed. (Recall that $\Verif$ might check model performance periodically.) 
\end{enumerate}


\renewcommand{\algorithmiccomment}[1]{$\triangleright$ #1}
\begin{algorithm}[htb]
\caption{Attack~II}
\label{alg:attack2}
\LinesNumbered 
\small
\KwIn {$D$, $f_{W_T}$, $\delta$, \textcolor{blue}{$\gamma$}, $\zeta$, $k$, $E$, $S$}
\KwOut {PoL spoof: $\Proof(\Adv, f_{W_T})=(\WWW, \III, \HHH)$ \\
~~~~~~~~~~~updated dataset: $D$}
$\WWW\leftarrow\{\}$
$\III\leftarrow\{\}$
$\HHH\leftarrow\{\}$

\textcolor{blue}{$\WWW.\texttt{append}(\texttt{initW}_0(\zeta, W_T))$}  \hfill\CommentSty{initialize and append $W_0$}


\For(\hfill\CommentSty{$T' \mod k =0$}){$t = 1  \to T' $}{


    $\III.\texttt{append}(\texttt{getBatch}(D))$
    


    \eIf{$t\mod k = 0$}{
        \eIf(\hfill\CommentSty{no need to append $W_T$}){$t < T'$}{
            \textcolor{blue}{~sample $W_{t}$ s.t., $d(W_{t}, W_{t-k})\leq {\delta}$}
            $\WWW.\texttt{append}(W_t)$ 
        }{
            $W_t := W_T$
        }
        
        $\texttt{updateDataPoints}(W_{t-k}, W_{t})$
        
        \For{$i = (t-k) \to (t-1)$}{
$\HHH.\texttt{append}(h(D[\III[i]]))$
}
    }{
        $\WWW.\texttt{append}(\mathbf{nil})$ 
    }
   
}


~
\SetKwFunction{FMain}{updateDataPoints}
\SetKwProg{Fn}{function}{}{end}

\Fn{\FMain{$W_{t-k}$, $W_{t}$}}{


    $W'_{t-k} := W_{t-k}$
    
    \For{$i = (t-k) \to (t-1)$}{
        $(\mathbf{X}, \mathbf{y}) \leftarrow D[\III[i]]$
        
        $W'_{i+1} \leftarrow \texttt{update}(W'_i, (\mathbf{X}, \mathbf{y}))$
        
\While(\hfill){\textcolor{blue}{$d(W'_{i+1}, W'_{i}) > \gamma$}}{
            $\mathbf{R} \leftarrow \texttt{zeros}$
            
            $\triangledown_{W'_i} \leftarrow - \frac{\partial}{\partial W'_i} L(f_{W'_i}(\mathbf{X}+\mathbf{R}), \mathbf{y})$
            
            \textcolor{blue}{ 
            $\mathbb{D}_i \leftarrow d(\triangledown_{W'_i} , 0)$ + $d(\mathbf{R}, 0)$
            }
            
            $\mathbf{R} \leftarrow \mathbf{R} - \eta' \triangledown_R \mathbb{D}_i$
            
            $W'_{i+1} \leftarrow \texttt{update}(W'_i, (\mathbf{X}+\mathbf{R}, \mathbf{y}))$
        }
        $D[\III[i]] := (\mathbf{X}+\mathbf{R}, \mathbf{y})$
    }
}
\end{algorithm}

Algorithm~\ref{alg:attack2} shows Attack~II. We highlight the key differences (compared to Attack~I) in blue.

This time, $\Adv$ initializes $W_0$ via $\texttt{initW}_0$ (line 2), which ensures that $W_0$ follows the given distribution $\zeta$,
and minimizes $d(W_0, W_T)$ at the same time. 
It works as follows:
\begin{enumerate}
    \item Suppose there are $n$ elements in $W_T$,
    $\Adv$ puts these elements into a set $S_1$. Then, $\Adv$ samples $n$ elements: $v_1, ..., v_n$ from the given distribution $\zeta$, and puts them into another set $V_2$.
\item $\Adv$ finds the largest elements $w$ and $v$ from $S_1$ and $S_2$ respectively.
    Then, $\Adv$ puts $v$ into $W_0$ according
    to $w$'s indices in $W_T$.
    \item $\Adv$ remove $(w, v)$ from $(S_1, S_2)$, and repeats step 2) until $S_1$ and $S_2$ are empty.
\end{enumerate}
Our experimental results show that this process can initialize a $W_0$ that meets our requirements. 

For other $W_t$s ($t>0$), $\Adv$ can initialize them by equally dividing the distance between $W_0$ and $W_T$.
If $T'$ is large enough (i.e., there are enough $W_t$s), the condition ``$d(W_t, W_{t-k}) \leq \delta$'' can be trivially satisfied.



Another major {\BeginAccSupp{ActualText=is}change\EndAccSupp{}} in Attack~II is that $\Adv$ optimizes the noise $\RR$ by minimizing the following distance (line 27):
\begin{center}
    $\mathbb{D}_i \leftarrow d(\triangledown_{W'_i} , 0)$ + $d(\mathbf{R}, 0)$,
\end{center}
and the condition for terminating the adversarial optimization is $d(W'_{i+1}, W'_i) >\gamma$ where $\gamma \ll \delta$ (Line 25).
This guarantees that the model is still close to itself after a single step of update.
Since the distance between $W_{t-k}$ and $W_{t}$ is smaller than $\delta$ after initialization, after $k$ steps of updates, their distance is still smaller than $\delta$: $d(W_t, W'_t) < \delta$.


Interestingly, this change makes the adversarial optimization become easier to converge. 
Recall that in Attack~I, $\Adv$ has to adjust the loss function $L(f_{W'_i}(\mathbf{X}+\mathbf{R}), \mathbf{y})$ to minimize $d(W'_{t-1}+\eta \triangledown_{W'_{t-1}}, W_{t})$. 
This is difficult to {\BeginAccSupp{ActualText=is}achieve\EndAccSupp{}} because gradient-based training is used to minimize (not adjust) the loss function.
Thanks to the new $\mathbb{D}_i$, $\Adv$ can simply minimize the loss function in Attack~II.
In another word, the adversarial optimization process in Attack~II is more close to {\BeginAccSupp{ActualText=is}normal\EndAccSupp{}} training.
Table~\ref{tab:loss-diff} shows that on CIFAR-10, after one step of adversarial optimization, the loss function {\BeginAccSupp{ActualText=is}decreases\EndAccSupp{}} from 0.43 to 0.04, and the gradients decrease from 61.13 to 0.12.
Both are small enough to pass the verification.
That is to say, $N$ can be as small as one in Attack~II.



\begin{table}[htb]
\centering
\caption{The changes of loss and the gradients after one step of adversarial optimization on CIFAR-10}
\begin{tabular}{@{}lll@{}}
\toprule
 & $L(f_{W'_i}(\mathbf{X}+\mathbf{R}), \mathbf{y})$ & $\left \|\triangledown_{W'_i}\right \|^{2}$ \\ \midrule
Before & 0.43 $\pm$ 0.18 &  61.13 $\pm$ 45.86 \\
After & 0.04 $\pm$ 0.01 & {0.12 $\pm$ 0.05}\\ \bottomrule
\end{tabular}
\label{tab:loss-diff}
\end{table}








However, Attack~II has to {\BeginAccSupp{ActualText=is}run\EndAccSupp{}} adversarial optimization for all $T'$ steps;
otherwise, a single step can make the model go far away from $W_T$.
As a result, the {\BeginAccSupp{ActualText=is}complexity\EndAccSupp{}} for Attack~II is \ul{$T'$ times of \texttt{update} plus $T' \cdot N$ times of adversarial optimization}.
We show how we {\BeginAccSupp{ActualText=is}reduce\EndAccSupp{}} this complexity in Attack~III. 



























































\subsection{Attack~III}

\renewcommand{\algorithmiccomment}[1]{$\triangleright$ #1}
\begin{algorithm}[htb]
\caption{Attack~III}
\label{alg:attack3}
\LinesNumbered 
\small
\KwIn {$D$, $f_{W_T}$, $\delta$, $\gamma$, $\zeta$, $k$, $E$, $S$}
\KwOut {PoL spoof: $\Proof(\Adv, f_{W_T})=(\WWW, \III, \HHH)$ \\
~~~~~~~~~~~updated dataset: $D$}
$\WWW\leftarrow\{\}$
$\III\leftarrow\{\}$
$\HHH\leftarrow\{\}$

$\WWW.\texttt{append}(\texttt{initW}_0(\zeta, W_T))$  \hfill\CommentSty{initialize and append $W_0$}


\For(\hfill\CommentSty{$T' \mod k =0$}){$t = 1  \to T' $}{


    $\III.\texttt{append}(\texttt{getBatch}(D))$
    
    \eIf{$t\mod k = 0$}{
        \eIf(\hfill\CommentSty{no need to append $W_T$}){$t < T$}{
            ~sample $W_{t}$ s.t., {$d(W_{t}, W_{t-k})\leq {\delta}$}
            $\WWW.\texttt{append}(W_t)$ 
        }{
            $W_t := W_T$
        }
        
        $\texttt{updateDataPoints}(W_{t-k}, W_{t})$
        
        \For{$i = (t-k) \to (t-1)$}{
            $\HHH.\texttt{append}(h(D[\III[i]]))$
        }
    }{
        $\WWW.\texttt{append}(\mathbf{nil})$ 
    }
}


~
\SetKwFunction{FMain}{updateDataPoints}
\SetKwProg{Fn}{function}{}{end}

\Fn{\FMain{$W_{t-k}$, $W_{t}$}}{




\textcolor{blue}{ 
        $(\mathbf{X}, \mathbf{y}) \leftarrow [D[\III[t-k]]...D[\III[t-1]]]$
        }
        
        $W'_{t} \leftarrow \texttt{update}(W_{t-k}, (\mathbf{X}, \mathbf{y}))$
               
        \While{$d(W'_{t}, W_{t}) > \gamma \textcolor{blue}{-\sigma}$}{
            
            $\mathbf{R} \leftarrow \texttt{zeros}$
            
            
            \textcolor{blue}{ 
            $\triangledown_{W_{t-k}} \leftarrow - \frac{\partial}{\partial W_{t-k}} L(f_{W_{t-k}}(\mathbf{X}+\mathbf{R}), \mathbf{y})$
            }
            
            \textcolor{blue}{ 
            $\mathbb{D}_{t-k} \leftarrow d(\triangledown_{W_{t-k}} , 0)$ + $d(\mathbf{R}, 0)$
            }
            
            $\mathbf{R} \leftarrow \mathbf{R} - \eta' \triangledown_R \mathbb{D}_{t-k}$
            
            \textcolor{blue}{ 
            $W'_{t} \leftarrow \texttt{update}(W_{t-k}, (\mathbf{X}+\mathbf{R}, \mathbf{y}))$
            }
        }
        \textcolor{blue}{ 
        $[D[\III[t-k]]...D[\III[t-1]]] := (\mathbf{X}+\mathbf{R}, \mathbf{y})$}
}
\end{algorithm}

Algorithm~\ref{alg:attack3} shows Attack III. Again, we highlight the key differences (compared to Attack II) in blue. 
The major change is that $\Adv$ optimizes all the $k$ batches of data points  together in \texttt{updateDataPoints}.
This reduces the complexity to \ul{$T'/k$ times of \texttt{update} plus $T'\cdot N /k$ times of adversarial optimization}.
At first glance, this will not pass the verification because $\Verif$ will run \texttt{update} for each batch individually. 
In fact, however, the gap only {\BeginAccSupp{ActualText=is}depends\EndAccSupp{}} on $k$, hence we can make a trade-off. 
We formally prove this argument in the rest of this section.
Our experimental results show that when we set  $k=100$, our {\BeginAccSupp{ActualText=is}spoof\EndAccSupp{}} can still pass the verification. 



 
\begin{corollary}
Let $(W_{t-k}, W_t)$ be an {\BeginAccSupp{ActualText=is}input\EndAccSupp{}} to \texttt{updateDataPoints} in Attack III.
Let $\{\hat{W}_{t-k-1},...,\hat{W}_{t}\}$ be the model weights computed by $\Verif$ {\BeginAccSupp{ActualText=is}based\EndAccSupp{}} on $W_{t-k}$ during PoL verification. 
Assuming the loss function $L(f_{W}(\mathbf{X}),\mathbf{y}) \in C^2(\Omega)$, where $\Omega$ is a closed, convex and connected subset in $\mathbb{R}^n$, and $\{\hat{W}_{t-k-1},...,\hat{W}_{t}\} \in \Omega$. Then, 
$$||\hat{W}_t-W_t||\leq \eta^2\alpha\beta \frac{(k-1)(k-2)}{2} + \gamma - \sigma,$$
where $\alpha$ and $\beta$ are the upper bounds of first and second order derivative\footnote{Empirically, $\alpha$ is 0.03 in average and $\beta$ is 0.025 in average.} of $L(f_{W}(\mathbf{X}),\mathbf{y})$.
\end{corollary}

\begin{proof}
Let $\XX=[\xx_1, \xx_2, ..., \xx_k]^T$ be the $k$ batches used to update $W_{t-k}$. 
Denote 
$$L_i(W) =L(f_W(\xx_i),\yy_i)\in C^2(\Omega),$$
$$\triangledown_i(W) = \frac{\partial}{\partial W}L_i\in C^1(\Omega),$$
$$\triangledown'_i(W) = \frac{\partial^2}{\partial W^2}L_i\in C^0(\Omega).$$
Then, $||\triangledown_i(W)||<\alpha$ and $||\triangledown'_i(W)||<\beta$.

In Attack III, (Line 22 of Algorithm~\ref{alg:attack3}), $W'_t$ is calculated as 
\begin{equation*}
    \begin{aligned}
    W'_t =&~W_{t-k}-\frac{\eta''}{k}(
       \triangledown_1(W_{t-k})+\triangledown_2(W_{t-k})+...+\triangledown_k(W_{t-k})
    )
    \end{aligned}
\end{equation*}
Whereas, in PoL verification (Line 29 of Algorithm~\ref{alg:verification}),
\begin{equation*}
    \begin{aligned}
    \hat{W}_{t-k+1} =&~W_{t-k}-\eta\triangledown_1(W_{t-k}) \\
    \hat{W}_{t-k+2} =&~\hat{W}_{t-k+1}-\eta\triangledown_2(\hat{W}_{t-k+1}) \\
    ...\\
    \hat{W}_t =&~\hat{W}_{t-1}-\eta\triangledown_k(\hat{W}_{t-1})
    \end{aligned}
\end{equation*}
It is identical to
\begin{equation*}
    \begin{aligned}
    \hat{W}_t = &W_{t-k}-\eta(
     \triangledown_1(W_{t-k})+\triangledown_2(\hat{W}_{t-k+1})+...+\triangledown_k(\hat{W}_{t-1})
       )
    \end{aligned}
\end{equation*}
If $\Adv$ sets $\eta'' = k\eta$, then
\begin{equation*}
    \begin{aligned}
       \hat{W}_t -W'_t = \eta[&(\triangledown_2(W_{t-k})-\triangledown_2(\hat{W}_{t-k+1})+\\&
       (\triangledown_3(W_{t-k})-\triangledown_3(\hat{W}_{t-k+2})+...+\\
       &(\triangledown_k(W_{t-k})-\triangledown_k(\hat{W}_{t-1})]
    \end{aligned}
\end{equation*}
Assuming $[\hat{W}_{t-k+l},W_{t-k}]=\{W\in \mathbb{R}^n,W=\hat{W}_{t-k+l}+\theta h,0\leq \theta \leq 1\}$ is a closed set, and $\triangledown_i(W)\in C^1(\Omega)$. 
{\BeginAccSupp{ActualText=is}Based\EndAccSupp{}} on the finite-increment theorem (Chapter 10, {\BeginAccSupp{ActualText=is}Section\EndAccSupp{}} 4 of \cite{book-1469653}), we have
    \begin{equation*}
    \begin{aligned}
||\triangledown_i(W_{t-k}) - \triangledown_i(\hat{W}_{t-k+l})||&\leq \sup_{W}||\frac{\partial\triangledown_i(W)}{\partial W}||\cdot||h|| \\&\leq  \beta||W_{t-k}-\hat{W}_{t-k+l}||
\end{aligned}
   \end{equation*}
Given 
    \begin{equation*}
    \begin{aligned}
||W_{t-k}-\hat{W}_{t-k+l}|| &=\eta ||\triangledown_1(W_{t-k}) + \triangledown_2(\hat{W}_{t-k+1}) + ... \\ &~~~~+  \triangledown_{l-1}(\hat{W}_{t-k+l-1})|| \\ 
&\leq (l-1) \eta \alpha,
        \end{aligned}
   \end{equation*}
we have
$$||\triangledown_i(W_{t-k}) - \triangledown_i(\hat{W}_{t-k+l})|| \leq 
(l-1)\eta \alpha \beta.$$
Then,
\begin{equation*}
\begin{aligned}
          \hat{W}_t -W'_t  &\leq 
          \eta^2\alpha\beta\sum_{l=1}^{k-1}{(l-1)} \\
          &= \eta^2\alpha\beta \frac{(k-1)(k-2)}{2}
\end{aligned}
\end{equation*}

Recall that $d(W'_t,W_t) \leq \gamma - \sigma$ (Line 23 in Algorithm~\ref{alg:attack3}). 
Then, 
\begin{equation*}
\begin{aligned}
    ||\hat{W}_t-W_t|| &= ||\hat{W}_t-W'_t+W'_t-W_t||\\
    &\leq ||\hat{W}_t-W'_t|| + ||W'_t-W_t||\\
    &\leq \eta^2\alpha\beta \frac{(k-1)(k-2)}{2} + \gamma - \sigma
    \end{aligned}
\end{equation*}













\end{proof}

Therefore, Attack III can pass the verification if we set $\sigma > \eta^2\alpha\beta \frac{(k-1)(k-2)}{2}$. 



\begin{figure*}
    \centering
    \subfigure[Normalized reproduction error in $l_1$]{
    \includegraphics[width=0.32\linewidth]{./pic/norm1_cifar10_attack2.png}\label{fig:2.1}}
    \subfigure[Normalized reproduction error in $l_2$]{
    \includegraphics[width=0.32\linewidth]{./pic/norm2_cifar10_attack2.png}\label{fig:2.2}}
    \subfigure[Normalized reproduction error in $l_{\infty}$]{
    \includegraphics[width=0.32\linewidth]{./pic/norminf_cifar10_attack2.png}\label{fig:2.3}}
    \\
    \subfigure[Normalized reproduction error in $cos$]{
    \includegraphics[width=0.32\linewidth]{./pic/normcos_cifar10_attack2.png}\label{fig:2.4}}
    \subfigure[Spoof generation time.]{
    \includegraphics[width=0.32\linewidth]{./pic/Time_cifar10_attack2.png}\label{fig:2.5}}
    \subfigure[Spoof size.]{
    \includegraphics[width=0.32\linewidth]{./pic/Size_cifar10_attack2.png}\label{fig:2.6}}        
\caption{Attack II on CIFAR-10}
    \label{fig:Attack2_cifar10_eps}
\end{figure*}
\begin{figure*}
    \centering
    \subfigure[Normalized reproduction error in $l_1$]{
    \includegraphics[width=0.32\linewidth]{./pic/norm1_cifar100_attack2.png}\label{fig:3.1}}
    \subfigure[Normalized reproduction error in $l_2$]{
    \includegraphics[width=0.32\linewidth]{./pic/norm2_cifar100_attack2.png}\label{fig:3.2}}
    \subfigure[Normalized reproduction error in $l_{\infty}$]{
    \includegraphics[width=0.32\linewidth]{./pic/norminf_cifar100_attack2.png}\label{fig:3.3}}
    \\
    \subfigure[Normalized reproduction error in $cos$]{
    \includegraphics[width=0.32\linewidth]{./pic/normcos_cifar100_attack2.png}\label{fig:3.4}}
    \subfigure[Spoof generation time.]{
    \includegraphics[width=0.32\linewidth]{./pic/Time_cifar100_attack2.png}\label{fig:3.5}}
    \subfigure[Spoof size.]{
    \includegraphics[width=0.32\linewidth]{./pic/Size_cifar100_attack2.png}\label{fig:3.6}}        
\caption{Attack II on CIFAR-100}
    \label{fig:Attack2_cifar100_eps}
\end{figure*}
\begin{figure*}
    \centering
    \subfigure[Normalized reproduction error in $l_1$.]{
    \includegraphics[width=0.32\linewidth]{./pic/norm1_cifar10_attack3.png}\label{fig:4.1}}
    \subfigure[Normalized reproduction error in $l_2$.]{
    \includegraphics[width=0.32\linewidth]{./pic/norm2_cifar10_attack3.png}\label{fig:4.2}}
    \subfigure[Normalized reproduction error in $l_{\infty}$.]{
    \includegraphics[width=0.32\linewidth]{./pic/norminf_cifar10_attack3.png}\label{fig:4.3}}
    \subfigure[Normalized reproduction error in $cos$.]{
    \includegraphics[width=0.32\linewidth]{./pic/normcos_cifar10_attack3.png}\label{fig:4.4}}
    \subfigure[Spoof generation time.]{
    \includegraphics[width=0.32\linewidth]{./pic/Time_cifar10_attack3.png}\label{fig:4.5}}
    \subfigure[Spoof Size.]{
    \includegraphics[width=0.32\linewidth]{./pic/Size_cifar100_attack3.png}\label{fig:4.6}}
    
    \caption{Attack III on CIFAR-10.}
    \label{fig:Attack3_cifar10_eps}
\end{figure*}
\begin{figure*}
    \centering
    \subfigure[Normalized reproduction error in $l_1$.]{
    \includegraphics[width=0.32\linewidth]{./pic/norm1_cifar100_attack3.png}\label{fig:5.1}}
    \subfigure[Normalized reproduction error in $l_2$.]{
    \includegraphics[width=0.32\linewidth]{./pic/norm2_cifar100_attack3.png}\label{fig:5.2}}
    \subfigure[Normalized reproduction error in $l_{\infty}$.]{
    \includegraphics[width=0.32\linewidth]{./pic/norminf_cifar100_attack3.png}\label{fig:5.3}}
    \subfigure[Normalized reproduction error in $cos$.]{
    \includegraphics[width=0.32\linewidth]{./pic/normcos_cifar100_attack3.png}\label{fig:5.4}}
    \subfigure[Spoof generation time.]{
    \includegraphics[width=0.32\linewidth]{./pic/Time_cifar100_attack3.png}\label{fig:5.5}}
    \subfigure[Spoof size.]{
    \includegraphics[width=0.32\linewidth]{./pic/Size_cifar100_attack3.png}\label{fig:5.6}}
    \caption{Attack III on CIFAR-100. }
    \label{fig:Attack3_cifar100_eps}
\end{figure*}
\begin{figure*}
    \centering
    \subfigure[Intermediate models accuracy on CIFAR-10]{
    \includegraphics[width=0.45\linewidth]{./pic/intermediate_model_accuracy.png}}
    \subfigure[Intermediate models accuracy on CIFAR-100]{
    \includegraphics[width=0.45\linewidth]{./pic/intermediate_model_cifar100_accuracy.png}}
    \caption{Intermediate model accuracy ($T'=300$) }
    \label{fig:intermediate_model_accuracyl}
\end{figure*}
\begin{figure*}
    \centering
    \subfigure[CIFAR-10]{
    \includegraphics[width=0.45\linewidth]{./pic/ad_opt1.PNG}}
    \subfigure[CIFAR-100]{
    \includegraphics[width=0.45\linewidth]{./pic/ad_opt2.PNG}}
    \caption{``Adversarial examples'' generated by Attack~III. The original images are on the left-hand side and the noised images are on the right-hand side.}
    \label{fig:ad_exp}
\end{figure*}

\section{Evaluation}
\label{sec:eval}

In this section, we empirically evaluate our attacks in two metrics:
\begin{itemize}
    \item {\bf Reproducibility.} We need to show that the {\BeginAccSupp{ActualText=is}normalized\EndAccSupp{}} {\BeginAccSupp{ActualText=is}reproduction\EndAccSupp{}} errors (cf. Section~\ref{sec:verification}) in $l_1$, $l_2$, $l_{\infty}$ and $cos$ introduced by our PoL {\BeginAccSupp{ActualText=is}spoof\EndAccSupp{}} $\Proof(\Adv, f_{W_T})$ are smaller than  those introduced by a legitimate PoL proof $\Proof(\Prov, f_{W_T})$.
    That means, as long as $\Proof(\Prov, f_{W_T})$ can pass the verification, our {\BeginAccSupp{ActualText=is}spoof\EndAccSupp{}} $\Proof(\Adv, f_{W_T})$ can pass the verification as well.
    \item {\bf Spoof cost.} Recall that a successful PoL {\BeginAccSupp{ActualText=is}spoof\EndAccSupp{}} requires $\Adv$ to spend less computation and storage than $\Prov$ (cf. Section~\ref{sec:definition}).
    Therefore, we need to show that the generation time and {\BeginAccSupp{ActualText=is}size\EndAccSupp{}} of $\Proof(\Adv, f_{W_T})$ are smaller than those of $\Proof(\Prov, f_{W_T})$.
    
\end{itemize}

\subsection{Setting}

Following the experimental setup in~\cite{PoL},
we evaluate our attacks for ResNet-20~\cite{He2016resnet} and ResNet-50~\cite{He2016resnet} on CIFAR-10~\cite{krizhevsky2009learning} and CIFAR-100~\cite{krizhevsky2009learning} respectively. 
Each of them {\BeginAccSupp{ActualText=is}contains\EndAccSupp{}} 50,000 training {\BeginAccSupp{ActualText=is}images\EndAccSupp{}} and 10,000 {\BeginAccSupp{ActualText=is}testing\EndAccSupp{}} images; and each {\BeginAccSupp{ActualText=is}image\EndAccSupp{}} is of {\BeginAccSupp{ActualText=is}size\EndAccSupp{}} 32×32×3. 
CIFAR-10 only has 10 {\BeginAccSupp{ActualText=is}classes\EndAccSupp{}} and CIFAR-100 has 100 classes.


We reproduced the results in~\cite{PoL} as baselines for our attacks. 
Namely, we generate $\Proof(\Prov, f_{W_T})$ by training both models for 200 epochs with 390 steps in each epoch (i.e., $E=200$, $S=390$) with batch {\BeginAccSupp{ActualText=is}sizes\EndAccSupp{}} being 128;
we set $k$ as 100\footnote{The authors in~\cite{PoL} suggest to set $k=S$ (which is 390), but in that case $Q$ can only be one.} and measure the normalized {\BeginAccSupp{ActualText=is}reproduction\EndAccSupp{}} errors and costs.
Since our attacks are stochastic spoofing, where $\Proof(\Adv, f_{W_T})$ is {\em not} required to be exactly the same as $\Proof(\Prov, f_{W_T})$, we could use different $T$, $k$ and batch size in our attacks.
Recall that $\Verif$ only verifies $Q < \frac{S}{k}$ largest updates for each epoch (cf. Algorithm~\ref{alg:verification}).
Therefore, we measure the reproducibility and {\BeginAccSupp{ActualText=is}spoof\EndAccSupp{}} cost for both $Q = \left\lfloor \frac{S}{k}  \right\rfloor$ (full verification) and $Q=1$ (top-Q verification).

We set $k$ as 100 and batch size as 10 for all of our attacks except Attack~III on CIFAR-100\footnote{The model for CIFAR-100 is large and Attack~III needs to load all $k$ batches into the memory. So we have to use a smaller $k$ or batch size.}.
Since it is difficult for Attack~I to converge, we focus on evaluating Attack~II and Attack~III.













\subsection{Attack II}




\fig~\ref{fig:Attack2_cifar10_eps} shows the evaluation results of Attack II on CIFAR-10. 
The results show that the normalized {\BeginAccSupp{ActualText=is}reproduction\EndAccSupp{}} errors introduced by $\Proof(\Adv, f_{W_T})$ are always smaller than those introduced by $\Proof(\Prov, f_{W_T})$ in $l_1$, $l_2$ and $l_{cos}$ (\fig~\ref{fig:2.1}, \ref{fig:2.2} and \ref{fig:2.4}).
For $l_{\infty}$, it requires $T'>15$ for $\Proof(\Adv, f_{W_T})$ to be able to pass the verification  (\fig~\ref{fig:2.3}).
On the other hand, when $T'>30$, the generation time of $\Proof(\Adv, f_{W_T})$ is {\BeginAccSupp{ActualText=is}larger\EndAccSupp{}} than that of $\Proof(\Prov, f_{W_T})$ (\fig~\ref{fig:2.5}).
That {\BeginAccSupp{ActualText=is}means\EndAccSupp{}} $15<T'<30$ would be the condition for Attack II to be successful on CIFAR-10.
Notice that the {\BeginAccSupp{ActualText=is}spoof\EndAccSupp{}} size is always smaller than the proof size.


\fig~\ref{fig:Attack2_cifar100_eps} shows the evaluation results of Attack II on CIFAR-100.
When $T'>40$, the normalized reproduction errors introduced by $\Proof(\Adv, f_{W_T})$ are smaller than those introduced by $\Proof(\Prov, f_{W_T})$ in all 4 distances.
On the other hand, it requires $T'<50$ for the {\BeginAccSupp{ActualText=is}spoof\EndAccSupp{}} generation time to be smaller than the baseline.
Therefore, $40<T'<50$ is the condition for Attack II to be successful on CIFAR-100.




















\subsection{Attack III}


The evaluation results of Attack III on CIFAR-10 are {\BeginAccSupp{ActualText=is}shown\EndAccSupp{}} in \fig~\ref{fig:Attack3_cifar10_eps}. 
The results show that the normalized reproduction errors introduced by $\Proof(\Adv, f_{W_T})$ are always smaller than those introduced by $\Proof(\Prov, f_{W_T})$ in all 4 distances when $T'>25$ (\fig~\ref{fig:4.1}-\ref{fig:4.4}).
In {\BeginAccSupp{ActualText=is}terms\EndAccSupp{}} of {\BeginAccSupp{ActualText=is}spoof\EndAccSupp{}} generation time, the number of steps $T'$ can be as large as 300 (\fig~\ref{fig:4.5}).
That means the condition for Attack III to be successful on CIFAR-100 is $25<T'<300$.


\fig~\ref{fig:Attack3_cifar100_eps} shows the evaluation results of Attack III on CIFAR-100. 
We set $k=100$ and batch size as 2, and evaluate the attack with different number of steps ($T'$).
The results show that the normalized reproduction errors introduced by $\Proof(\Adv, f_{W_T})$ are always smaller than those introduced by $\Proof(\Prov, f_{W_T})$ in $l_{\infty}$ and $cos$ (\fig~\ref{fig:5.3} and \fig~\ref{fig:5.4}).
The cross-over point is around 100 in both $l_1$ and $l_2$: when $T'>100$, the normalized reproduction errors introduced by $\Proof(\Adv, f_{W_T})$ are smaller than those introduced by $\Proof(\Prov, f_{W_T})$ in $l_1$ and $l_2$ (\fig~\ref{fig:5.1} and \fig~\ref{fig:5.2}).
Same as on CIFAR-10, the {\BeginAccSupp{ActualText=is}spoof\EndAccSupp{}} generation time is smaller than the proof generation time when $T'<300$.
Therefore, the condition for Attack III to succeed in CIFAR-100 is $100<T'<300$.



In~\cite{PoL}, the authors {\BeginAccSupp{ActualText=is}suggest\EndAccSupp{}} to check model performance periodically.
Therefore, we need to make sure that the performance of the intermediate models generated by our attacks follow the same tend as those in  $\Proof(\Prov, f_{W_T})$.
We can achieve this by adjusting the extent of {\BeginAccSupp{ActualText=is}perturbations\EndAccSupp{}} on $W_T$ (Line 7 in Algorithm~\ref{alg:attack3}).
Specifically, we can add a large extent of {\BeginAccSupp{ActualText=is}perturbations\EndAccSupp{}} when $T'$ is small and add a small extent of perturbations when $T'$ is large.
\fig~\ref{fig:intermediate_model_accuracyl} shows the model performance in both CIFAR-10 and CIFAR-100.
The $x$-axis presents the progress of training. 
For example when $x=0.2$, the corresponding $y$ represents the $0.2\cdot T$-th model performance in $\Proof(\Prov, f_{W_T})$ and $0.2\cdot T'$-th model performance $\Proof(\Adv, f_{W_T})$.
It shows that the model performance in $\Proof(\Prov, f_{W_T})$ and $\Proof(\Adv, f_{W_T})$ are in similar trends.


\fig~\ref{fig:ad_exp} shows some randomly picked {\BeginAccSupp{ActualText=is}images\EndAccSupp{}} before and after running our attacks.
The differences are invisible.

\subsection{Summary}

In summary, both Attack~II and Attack~III can successfully {\BeginAccSupp{ActualText=is}spoof\EndAccSupp{}} a PoL when $T'$ is large enough.
We further remark that our attacks are slow mostly because they are not well-engineered as normal training, e.g., the optimizers in Tensorflow~\cite{Abadi2016tensorflow} and Pytorch~\cite{Paszke2019PyTorch} adopt a bunch of tricks to accelerate training.
Therefore, the number of steps $T'$ could be potentially increased, making our attack more imperceptible.

%
 
\section{Countermeasures}
\label{sec:discuss}

In this section, we provide some potential {\BeginAccSupp{ActualText=is}countermeasures\EndAccSupp{}} for our attacks.
However, even with these countermeasures, we still cannot guarantee a PoL mechanism is {\BeginAccSupp{ActualText=is}secure\EndAccSupp{}} without a formal proof.
Indeed, these {\BeginAccSupp{ActualText=is}countermeasures\EndAccSupp{}} have their own {\BeginAccSupp{ActualText=is}limitations\EndAccSupp{}} and can be broken.

\Paragraph{Model distance.}
Recall that Attack~II and Attack~III require:
\begin{center}
    $d(W_t, W_{t-k}) \leq \delta$.
\end{center}
However, in our experiments, we {\BeginAccSupp{ActualText=is}found\EndAccSupp{}} that such distances in real training are usually larger. 
Therefore, $\Verif$ could set a bound for the distance between each $(W_t, W_{t-k})$.
Notice that this {\BeginAccSupp{ActualText=is}countermeasure\EndAccSupp{}} is invalid for Attack I.






\Paragraph{Integrity of training dataset.}
In~\cite{PoL}, it is assumed that $\Adv$ has full access to the training {\BeginAccSupp{ActualText=is}dataset\EndAccSupp{}} $D$ and can modify it. 
If $\Adv$ has full access to $D$, most probably $D$ is a {\BeginAccSupp{ActualText=is}public\EndAccSupp{}} dataset. 
Then, $\Verif$ can easily verify the integrity of $D$ so that $\Adv$ cannot modify it.
If $D$ is a private dataset, then $\Adv$ can neither access nor modify it.
That means we could change the threat model of PoL to make the attack more difficult.
However, $\Adv$ could still {\BeginAccSupp{ActualText=is}spoof\EndAccSupp{}} $W_T$ using a totally different {\BeginAccSupp{ActualText=is}dataset\EndAccSupp{}} $D'$ and claim that $D'$ is the real training dataset of $W_T$. 





\Paragraph{Checkpointing interval and batch size.}
Recall that Attack~III for CIFAR-100 has to use either a small $k$ or a small batch size due to memory limitation. 
Then, $\Verif$ could set bounds for both $k$ and the batch size.
However, this {\BeginAccSupp{ActualText=is}countermeasure\EndAccSupp{}} is invalid for either Attack~I or Attack~II.
When $\Adv$ has a large GPU memory, it is invalid for Attack~III either.



\Paragraph{Number of training steps.}
Recall that Attack~II requires $T'<30$ to succeed and Attack~III requires $T'<300$. 
Then, $\Verif$ could set a bound for the number of training steps. 
However, as we mentioned, our attacks are slow mostly because they are not well-engineered as normal training.
We could potentially accelerate our attacks using the tricks adopted in the state-of-the-art optimizers.
Then, bounding the number of of training steps will be no longer useful.
Furthermore, this {\BeginAccSupp{ActualText=is}countermeasure\EndAccSupp{}} is invalid for Attack~I.


\Paragraph{Selection of threshold.}
As we {\BeginAccSupp{ActualText=is}observed\EndAccSupp{}} in our experiments, $\varepsilon_{\mathit{repr}}$ in the early training {\BeginAccSupp{ActualText=is}stage\EndAccSupp{}} is usually larger than that in the later stage, because the model converges in the later stage. 
On the other hand, $\varepsilon_{\mathit{repr}}$ {\BeginAccSupp{ActualText=is}remains\EndAccSupp{}} in the same {\BeginAccSupp{ActualText=is}level\EndAccSupp{}} in our spoof.
Then, it is unreasonable for $\Verif$ to set a single verification {\BeginAccSupp{ActualText=is}threshold\EndAccSupp{}} for the whole training stage.
A more sophisticated way would be dynamically {\BeginAccSupp{ActualText=is}choosing\EndAccSupp{}} the verification {\BeginAccSupp{ActualText=is}thresholds\EndAccSupp{}} according to the stage of model training:  choose larger thresholds for the early stage, and choose smaller thresholds for the later stage.
In that case, it will be more challenging for our {\BeginAccSupp{ActualText=is}spoof\EndAccSupp{}} to pass the verification.
However, we can also set $d(W_t, W_{t-k})$ closer in the later stage to circumvent this countermeasure. 
\section{Related Work}
\label{sec:related}

\subsection{Adversarial examples}

When first discovered in 2013~\cite{intri2013}, adversarial examples are {\BeginAccSupp{ActualText=is}images\EndAccSupp{}} designed intentionally to cause {\BeginAccSupp{ActualText=is}deep\EndAccSupp{}} {\BeginAccSupp{ActualText=is}neural\EndAccSupp{}} {\BeginAccSupp{ActualText=is}networks\EndAccSupp{}} to make false predictions. 
Such adversarial examples look almost the same as  original images, thus they show vulnerabilities of {\BeginAccSupp{ActualText=is}deep\EndAccSupp{}} neural networks~\cite{goodfellow2014explaining}. 
Since then, it becomes a popular {\BeginAccSupp{ActualText=is}research\EndAccSupp{}} topic and has been {\BeginAccSupp{ActualText=is}explored\EndAccSupp{}} extensively in both attacks~\cite{dong2018boosting, su2019one} and defences~\cite{papernot2016distillation,buckman2018thermometer,Bhagoji2018Enhancing,zheng2016improving,wang2017learning,luo2016foveationbased}. 
In particular, adversarial examples have been found in many {\BeginAccSupp{ActualText=is}domains\EndAccSupp{}} other than images, such as {\BeginAccSupp{ActualText=is}autonomous\EndAccSupp{}} driving~\cite{lu2017adversarial,lu2017need}, {\BeginAccSupp{ActualText=is}malware\EndAccSupp{}} detection~\cite{Grosse2017adversarial,Rosenberg2018Generic}, authentication~\cite{zhou2018invisible} and so on.
It becomes a major concern when considering {\BeginAccSupp{ActualText=is}safety\EndAccSupp{}} issues.







Roughly speaking, adversarial examples are generated by solving:
$$\mathbf{R}=\mathop{\arg\min}_{\mathbf{R}}L(f_{W}(\mathbf{X}+\mathbf{R}),\mathbf{y}')+\alpha||\mathbf{R}||,$$
where $\mathbf{y}'$ is a {\BeginAccSupp{ActualText=is}label\EndAccSupp{}} that is different from the real {\BeginAccSupp{ActualText=is}label\EndAccSupp{}} $\mathbf{y}$ for $\mathbf{X}$.
Then, the noise $\mathbf{R}$ can fool the model to predict a wrong {\BeginAccSupp{ActualText=is}label\EndAccSupp{}} (by minimizing the loss function) and pose little influence on the original instance $\mathbf{X}$.





Recall that the objective of Attack~II and Attack~III is to minimize the gradients computed by ``adversarial examples''. 
Therefore, it can be {\BeginAccSupp{ActualText=is}formulated\EndAccSupp{}} as: $$\mathbf{R}=\mathop{\arg\min}_{\mathbf{R}}L(f_{W}(\mathbf{X}+\mathbf{R}),\mathbf{y})+\alpha||\mathbf{R}||.$$
This is identical to the objective of finding an adversarial example.
An adversarial example aims to fool the model {\BeginAccSupp{ActualText=is}whereas\EndAccSupp{}} our attacks aim to update a model to itself.
Nevertheless, they end up at the same point, which explains the {\BeginAccSupp{ActualText=is}effectiveness\EndAccSupp{}} of Attack II and Attack III.







{\BeginAccSupp{ActualText=is}Goodfellow\EndAccSupp{}} {\BeginAccSupp{ActualText=is}et\EndAccSupp{}} al.~\cite{goodfellow2014explaining} proposed a one-step {\BeginAccSupp{ActualText=is}method\EndAccSupp{}} called fast gradient sign {\BeginAccSupp{ActualText=is}method\EndAccSupp{}} (FGSM) to generate adversarial examples:
$$\XX=\XX + \epsilon sign(\triangledown_{\XX} L(f_W(\mathbf{X}),\mathbf{y})),$$
where $\epsilon sign(\triangledown_{\XX} L(f_W(\mathbf{X}),\mathbf{y}))$ is the max-norm constrained perturbation and it can be computed by one step back-propagation.
Therefore, FGSM is much more efficient. 





Su {\BeginAccSupp{ActualText=is}et\EndAccSupp{}} al.~\cite{su2019one} {\BeginAccSupp{ActualText=is}target\EndAccSupp{}} a limited {\BeginAccSupp{ActualText=is}scenario\EndAccSupp{}} where only one pixel can be modified. They use $l_0$ norm as a constraint $||e(\mathbf{x})||_0\leq d$ to modify limited pixels. 
In their experiments, they choose $d=1$ so that only one pixel can be modified. 
Then, the objective function is: $$\mathop{\arg\max}_{e(\mathbf{\XX})}L(f(\mathbf{\XX}+e(\mathbf{\XX})),\mathbf{y}).$$ 
Their work show that more than 60$\%$ of the CIFAR-10 {\BeginAccSupp{ActualText=is}images\EndAccSupp{}} can be perturbed by their method.

We leave it as future work to {\BeginAccSupp{ActualText=is}estimate\EndAccSupp{}} the {\BeginAccSupp{ActualText=is}effectiveness\EndAccSupp{}} of these works in our settings.

\subsection{Deep Leakage from Gradient}

In federated learning~\cite{mcmahan17a, bonawitz2019towards, li2020federated, kairouz2019advances}), 
it was widely believed that shared gradients will not leak {\BeginAccSupp{ActualText=is}information\EndAccSupp{}} about the training data.
However, Zhu et al.~\cite{zhu2020deepleakage} proposed ``Deep Leakage from Gradients'' (DLG), where the training data can be recovered through gradients matching.
Specifically, after receiving gradients from another worker, the adversary feeds a pair of randomly initialized dummy instance $(X, y)$ into the model, and obtains the dummy gradients via back-propagation.
Then, they update the dummy instance with an objective of minimizing the distance between the dummy gradients and the received gradients:
\begin{equation*}
    \begin{aligned}
  \XX, y &= \mathop{\arg\min}_{\XX, y}||\triangledown W'-\triangledown W||^2\\&= \mathop{\arg\min}_{\XX, y}||\frac{\partial L(f_W(\XX), y)}{\partial W}-\triangledown W||^2
    \end{aligned}
\end{equation*}
After a certain number of steps, the dummy instance can be recovered to the training data of the {\BeginAccSupp{ActualText=is}worker\EndAccSupp{}} who sent the gradients. 


Our attacks were largely inspired by DLG: 
an instance can be updated so that its {\BeginAccSupp{ActualText=is}output\EndAccSupp{}} gradients can {\BeginAccSupp{ActualText=is}match\EndAccSupp{}} the given gradients.
The difference between DLG and our work is that DLG aims to recover the training data from the gradients, whereas we want to create a perturbation on a real instance to generate {\BeginAccSupp{ActualText=is}specific\EndAccSupp{}} gradients.

\subsection{Model stealing}

One goal of PoL is to resolve the dispute caused by model stealing attacks, 
which allow adversaries to {\BeginAccSupp{ActualText=is}learn\EndAccSupp{}} a new model $\hat{f}$ that is close to the {\BeginAccSupp{ActualText=is}target\EndAccSupp{}} model $f$.
Tramer et al.~\cite{tramer2016stealing} proposed the first model stealing attack that leverages the outputs from the {\BeginAccSupp{ActualText=is}target\EndAccSupp{}} model.
They demonstrate that for a large variety of machine {\BeginAccSupp{ActualText=is}learning\EndAccSupp{}} models, their  model {\BeginAccSupp{ActualText=is}extraction\EndAccSupp{}} attacks can successfully  ``steal'' the weights of the target model.
Orekondy et al.~\cite{orekondy2019knockoff} proposed a two-step model stealing attack.
By {\BeginAccSupp{ActualText=is}querying\EndAccSupp{}} data and using the output returned by the target model, they train a ``knockoff'' model $F_A$ to {\BeginAccSupp{ActualText=is}approximate\EndAccSupp{}} the target model. 
They show that their ``knockoff'' model achieves about 80$\%$ {\BeginAccSupp{ActualText=is}accuracy\EndAccSupp{}} of the target model.

Besides model weights, hyperparameters are also important for a machine {\BeginAccSupp{ActualText=is}learning\EndAccSupp{}} model.
Wang et al.~\cite{wang2018stealing} came up with an attack for stealing  hyperparameters. 
Given access to training data, objective function and model weights, 
they use a linear {\BeginAccSupp{ActualText=is}square\EndAccSupp{}} method to solve the overdetermined linear {\BeginAccSupp{ActualText=is}system\EndAccSupp{}} of the hyperparameters.
There are also some work to attack the {\BeginAccSupp{ActualText=is}architecture\EndAccSupp{}} and the optimization process of the target model~\cite{oh2019towards} under the assumption that adversary can only access to the {\BeginAccSupp{ActualText=is}query\EndAccSupp{}} {\BeginAccSupp{ActualText=is}inputs\EndAccSupp{}} and outputs. 


\subsection{Byzantine workers}

Another goal of PoL is to prevent {\BeginAccSupp{ActualText=is}Byzantine\EndAccSupp{}} {\BeginAccSupp{ActualText=is}workers\EndAccSupp{}} from conducting denial-of-service attacks in federated learning. 
A {\BeginAccSupp{ActualText=is}Byzantine\EndAccSupp{}} {\BeginAccSupp{ActualText=is}worker\EndAccSupp{}} can manipulate its gradients to prevent the convergence of the global model.
{\BeginAccSupp{ActualText=is}Blanchard\EndAccSupp{}} et al.~\cite{NIPS2017_f4b9ec30} prove that no aggregation rule based on linear combination (e.g., average) of the updates can tolerate a single Byzantine participant.
As a result, most {\BeginAccSupp{ActualText=is}researchers\EndAccSupp{}} working on this {\BeginAccSupp{ActualText=is}direction\EndAccSupp{}} are exploring alternative aggregation rules.
For example, in the {\BeginAccSupp{ActualText=is}Krum\EndAccSupp{}} aggregation rule~\cite{NIPS2017_f4b9ec30}, pairwise distances are calculated between all inputs submitted in an iteration, and picks the one with the lowest sum as the aggregated gradients.
Unfortunately, this line of work assumes that participants' training data are i.i.d., i.e., following the same distribution. 


\subsection{Verifiable computation}

In general, verifiable computation allows a delegator to outsource the execution of a complex function to some workers, and the delegator verifies the correctness of the returned result while performing less work than executing the function itself. 
A verifiable computation system for an NP relationship $R$ is a protocol between a computationally bounded prover and a verifier. 
At the end of the protocol, the verifier is convinced by prover that there exists a witness $w$ such that $(x; w) \in R$ for some input $x$. 
The correctness property of verifiable computation guarantees that an honest prover can always pass the verification, and the soundness guarantees that a cheating prover will be caught with overwhelming probability. 
There are many {\BeginAccSupp{ActualText=is}solutions\EndAccSupp{}} for verifiable computation such as SNARK~\cite{Pinocchio,libsnark}, STARK~\cite{libstark} etc.
We refer to~\cite{walfish2015verifying} for a {\BeginAccSupp{ActualText=is}complete\EndAccSupp{}} survey.

PoL is definitely a problem of verifiable computation. We leave it as future work to investigate how to design a {\BeginAccSupp{ActualText=is}secure\EndAccSupp{}} and {\BeginAccSupp{ActualText=is}efficient\EndAccSupp{}} PoL mechanism using verifiable computation.  

\section{Conclusion}
\label{sec:conc}

In this paper, we show that a recently proposed PoL mechanism is vulnerable to ``adversarial examples''.
Namely, in a similar way as generating adversarial examples, we could generate a PoL {\BeginAccSupp{ActualText=is}spoof\EndAccSupp{}}  with significantly less cost than generating a proof by the prover.
We validated our attacks by conducting  {\BeginAccSupp{ActualText=is}experiments\EndAccSupp{}} extensively. 
In future work, we will explore more effective attacks. 
We will also design a PoL mechanism that is both {\BeginAccSupp{ActualText=is}secure\EndAccSupp{}} and efficient.




%
 

\bibliographystyle{plain}
\bibliography{references}









\end{document}
